#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
è„šæœ¬å: correlation_analyzer.py
åŠŸèƒ½: ABACUS STRU è½¨è¿¹åˆ†æç›¸å…³æ€§åˆ†æå™¨
==================================================

åŠŸèƒ½ç‰¹æ€§ï¼š
---------
âœ¨ ä¸“ä¸šçš„ç»Ÿè®¡åˆ†ææ¨¡å—ï¼Œç”¨äºåˆ†æ ABACUS STRU è½¨è¿¹æ•°æ®
ğŸ”¬ ç§‘å­¦ä¸¥è°¨çš„ç»Ÿè®¡æ–¹æ³•ï¼Œç¡®ä¿ç»“æœå¯é æ€§
ğŸ“Š æ™ºèƒ½çš„æ ·æœ¬é‡æ£€æŸ¥å’Œè´¨é‡æ§åˆ¶
ğŸ›¡ï¸ ç¨³å¥çš„åˆ†æç­–ç•¥ï¼Œé¿å…å°æ ·æœ¬åè¯¯

ä½¿ç”¨æ–¹å¼ï¼š
---------
1. ç‹¬ç«‹è„šæœ¬è¿è¡Œï¼šåˆ†ææŒ‡å®šçš„ CSV æ–‡ä»¶
2. æ¨¡å—è°ƒç”¨ï¼šé›†æˆåˆ°ä¸»ç¨‹åºåˆ†ææµç¨‹

è¾“å…¥è¦æ±‚ï¼š
---------
- system_metrics_summary.csv æ–‡ä»¶
- åŒ…å«åˆ†å­IDã€æ„è±¡ã€æ¸©åº¦å’Œå„é¡¹æŒ‡æ ‡

è¾“å‡ºç»“æœï¼š
---------
- parameter_analysis_results.csvï¼šå…¨å±€åˆ†æè¯¦ç»†ç»“æœ
- parameter_analysis_summary.csvï¼šå…¨å±€åˆ†ææ±‡æ€»
- correlation_analysis.logï¼šåˆ†ææ—¥å¿—

æ ¸å¿ƒç‰¹æ€§ï¼š
---------
- å…¨å±€æ¸©åº¦ç›¸å…³æ€§åˆ†æï¼ˆå¤§æ ·æœ¬ï¼Œé«˜å¯é æ€§ï¼‰
- å…¨å±€æ„è±¡æ•ˆåº”åˆ†æï¼ˆè·¨æ‰€æœ‰åˆ†å­å’Œæ¸©åº¦ï¼‰
- å®Œæ•´çš„ç»Ÿè®¡æ£€éªŒå’Œæ•ˆåº”é‡è¯„ä¼°
- ç®€åŒ–çš„è¾“å‡ºæ ¼å¼ï¼Œèšç„¦å…¨å±€ç»“æœ

ä½œè€…ï¼šLoveElysia1314
ç‰ˆæœ¬ï¼šv3.0
æ—¥æœŸï¼š2025å¹´8æœˆ16æ—¥
æ›´æ–°ï¼šé‡æ„ä¸ºç‹¬ç«‹æ¨¡å—ï¼Œå¢å¼ºç»Ÿè®¡ç¨³å¥æ€§
"""

import os
import sys
import csv
import argparse
import logging
from typing import Optional, List, Dict, Tuple

import pandas as pd
import numpy as np
from scipy import stats
import shutil

# å¯¼å…¥å·¥å…·æ¨¡å—
from ..utils import (
    LoggerManager, FileUtils, DataUtils, MathUtils, ValidationUtils, 
    Constants, create_standard_logger
)



class CorrelationAnalyzer:
    """ç›¸å…³æ€§åˆ†æå™¨ç±»"""
    
    def __init__(self, logger: Optional[logging.Logger] = None):
        """
        åˆå§‹åŒ–ç›¸å…³æ€§åˆ†æå™¨
        
        Args:
            logger: æ—¥å¿—è®°å½•å™¨ï¼Œå¦‚æœä¸ºNoneåˆ™åˆ›å»ºé»˜è®¤æ—¥å¿—è®°å½•å™¨
        """
        self.logger = logger if logger is not None else self._create_default_logger()
        
        # å®šä¹‰åˆ†ææŒ‡æ ‡
        self.indicators = [
            'nRMSF', 'MCV', 'avg_nLdRMS', 
            'nRMSF_sampled', 'MCV_sampled', 'avg_nLdRMS_sampled'
        ]
    
    def _create_default_logger(self) -> logging.Logger:
        """åˆ›å»ºé»˜è®¤æ—¥å¿—è®°å½•å™¨"""
        return LoggerManager.create_logger(
            name='CorrelationAnalyzer',
            level=logging.INFO,
            add_console=True,
            log_format=Constants.DEFAULT_LOG_FORMAT,
            date_format=Constants.DEFAULT_DATE_FORMAT
        )
    
    def _to_python_list(self, seq):
        """å°†åºåˆ—ä¸­çš„numpyç±»å‹è½¬ä¸ºPythonåŸç”Ÿç±»å‹"""
        return DataUtils.to_python_types(seq)
    
    def analyze_correlations(self, csv_file_path: str, output_dir: str) -> bool:
        """
        åˆ†æåˆå§‹æ„è±¡ã€æ¸©åº¦ä¸å„æŒ‡æ ‡çš„ç›¸å…³æ€§
        
        Args:
            csv_file_path: system_metrics_summary.csv æ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•è·¯å¾„
            
        Returns:
            bool: åˆ†ææ˜¯å¦æˆåŠŸå®Œæˆ
        """
        try:
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            FileUtils.ensure_dir(output_dir)
            
            # å¦‚æœä½¿ç”¨å¤–éƒ¨loggerï¼Œä¸ºç›¸å…³æ€§åˆ†æåˆ›å»ºé¢å¤–çš„æ–‡ä»¶å¤„ç†å™¨
            file_handler = None
            if hasattr(self.logger, 'name') and self.logger.name != 'CorrelationAnalyzer':
                # ä½¿ç”¨å¤–éƒ¨loggeræ—¶ï¼Œæ·»åŠ æ–‡ä»¶è®°å½•åˆ°analysis_resultsç›®å½•
                analysis_results_dir = os.path.join(os.getcwd(), "analysis_results")
                os.makedirs(analysis_results_dir, exist_ok=True)
                log_file = os.path.join(analysis_results_dir, "correlation_analysis.log")
                file_handler = LoggerManager.add_file_handler(
                    self.logger, log_file,
                    Constants.DEFAULT_LOG_FORMAT, 
                    Constants.DEFAULT_DATE_FORMAT
                )
            
            # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not ValidationUtils.validate_file_exists(csv_file_path):
                self.logger.error(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {csv_file_path}")
                return False
            
            # è¯»å–CSVæ•°æ®
            df = pd.read_csv(csv_file_path)
            self.logger.info(f"æˆåŠŸè¯»å–æ•°æ®æ–‡ä»¶: {csv_file_path}")
            self.logger.info("å¼€å§‹è¿›è¡Œç›¸å…³æ€§åˆ†æï¼ˆéµå¾ªå•ä¸€å˜é‡åŸåˆ™ï¼‰...")
            
            # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
            required_columns = ['Configuration', 'Temperature(K)'] + self.indicators
            missing_columns = DataUtils.check_required_columns(df, required_columns)
            if missing_columns:
                self.logger.error(f"CSVæ–‡ä»¶ç¼ºå°‘å¿…è¦çš„åˆ—: {missing_columns}")
                return False
            
            # æ£€æŸ¥æ˜¯å¦æœ‰Molecule_IDåˆ—ï¼Œå¦‚æœæ²¡æœ‰åˆ™æ·»åŠ è­¦å‘Š
            if 'Molecule_ID' not in df.columns:
                self.logger.warning("æœªæ‰¾åˆ°Molecule_IDåˆ—ï¼Œå°†å‡è®¾æ‰€æœ‰æ•°æ®æ¥è‡ªåŒä¸€åˆ†å­")
                df['Molecule_ID'] = 'Unknown'
            
            # è·å–æ‰€æœ‰åˆ†å­ã€æ„è±¡å’Œæ¸©åº¦ï¼ˆä¿æŒåŸå§‹ç±»å‹ï¼Œé¿å…å¼ºåˆ¶è½¬ä¸ºintå¯¼è‡´é‡å¤æˆ–æ··ä¹±ï¼‰
            molecules = sorted(df['Molecule_ID'].unique())
            configs = sorted(df['Configuration'].unique())
            temperatures = sorted(df['Temperature(K)'].unique())
            
            self.logger.info(f"å‘ç° {len(molecules)} ä¸ªåˆ†å­ã€{len(configs)} ç§æ„è±¡ç¼–å·å’Œ {len(temperatures)} ç§æ¸©åº¦")
            self.logger.info(f"åˆ†å­: {self._to_python_list(molecules)}")
            self.logger.info(f"æ„è±¡ç¼–å·: {self._to_python_list(configs)}")
            self.logger.info(f"æ¸©åº¦: {self._to_python_list(temperatures)}K")
            
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            os.makedirs(output_dir, exist_ok=True)
            
            # æ‰§è¡Œå…¨å±€åˆ†æï¼ˆç§»é™¤åˆ†å­çº§åˆ†æï¼‰
            global_temp_results = self._analyze_global_temperature_correlations(df)
            global_config_results = self._analyze_global_configuration_effects(df)
            
            # ä¿å­˜ç»“æœï¼ˆä¼ å…¥ç©ºåˆ—è¡¨æ›¿ä»£åˆ†å­çº§ç»“æœï¼‰
            self._save_results([], [], global_temp_results, global_config_results, output_dir)
            self.logger.info(f"ç›¸å…³æ€§åˆ†æç»“æœå·²ä¿å­˜åˆ° {output_dir}")
            
            # è¾“å‡ºæ€»ç»“ï¼ˆä¼ å…¥ç©ºåˆ—è¡¨æ›¿ä»£åˆ†å­çº§ç»“æœï¼‰
            self._log_summary([], [], global_temp_results, global_config_results)
            
            # æ¸…ç†æ·»åŠ çš„æ–‡ä»¶å¤„ç†å™¨
            if file_handler:
                LoggerManager.remove_handler(self.logger, file_handler)
            
            return True
            
        except Exception as e:
            self.logger.error(f"ç›¸å…³æ€§åˆ†æå‡ºé”™: {str(e)}")
            import traceback
            self.logger.error(traceback.format_exc())
            
            # æ¸…ç†æ·»åŠ çš„æ–‡ä»¶å¤„ç†å™¨
            if 'file_handler' in locals() and file_handler:
                LoggerManager.remove_handler(self.logger, file_handler)
            
            return False
    
    def _analyze_global_temperature_correlations(self, df: pd.DataFrame) -> List[Dict]:
        """åˆ†ææ¸©åº¦ç›¸å…³æ€§ï¼ˆæŒ‰å•å˜é‡æ§åˆ¶åŸåˆ™ï¼šå›ºå®šåˆ†å­å’Œæ„è±¡ï¼Œåˆ†ææ¸©åº¦æ•ˆåº”ï¼‰"""
        global_temp_results = []
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ¸©åº¦å˜åŒ–
        unique_temps = df['Temperature(K)'].unique()
        total_systems = len(df)
        
        if not ValidationUtils.validate_sample_size(unique_temps, min_size=2):
            self.logger.warning(f"æ¸©åº¦ç§ç±»ä¸è¶³({len(unique_temps)}<2)ï¼Œè·³è¿‡æ¸©åº¦ç›¸å…³æ€§åˆ†æ")
            return global_temp_results
        
        # æŒ‰(åˆ†å­,æ„è±¡)åˆ†ç»„ï¼Œåªä¿ç•™æ ·æœ¬æ•°>=2çš„ç»„
        valid_groups = []
        filtered_groups = []
        
        for mol_id in df['Molecule_ID'].unique():
            mol_data = df[df['Molecule_ID'] == mol_id]
            for config in mol_data['Configuration'].unique():
                group_data = mol_data[mol_data['Configuration'] == config]
                group_data = DataUtils.clean_dataframe(group_data, ['Temperature(K)'] + self.indicators)
                
                if len(group_data) >= 2:
                    valid_groups.append({
                        'molecule': mol_id,
                        'config': config,
                        'data': group_data,
                        'size': len(group_data)
                    })
                else:
                    filtered_groups.append({
                        'molecule': mol_id,
                        'config': config,
                        'size': len(group_data)
                    })
        
        total_valid_samples = sum(group['size'] for group in valid_groups)
        total_filtered_samples = sum(group['size'] for group in filtered_groups)
        
        if not valid_groups:
            self.logger.warning("æ²¡æœ‰æ‰¾åˆ°æ ·æœ¬æ•°>=2çš„(åˆ†å­,æ„è±¡)ç»„ï¼Œè·³è¿‡æ¸©åº¦ç›¸å…³æ€§åˆ†æ")
            return global_temp_results
        
        # è®°å½•åˆ†ç»„ä¿¡æ¯
        self.logger.info(f"æ¸©åº¦åˆ†æåˆ†ç»„æƒ…å†µ:")
        self.logger.info(f"  æœ‰æ•ˆç»„: {len(valid_groups)}ä¸ª")
        if filtered_groups:
            self.logger.info(f"  è¿‡æ»¤ç»„: {len(filtered_groups)}ä¸ª(ç»„å†…æ ·æœ¬ä¸è¶³2ä¸ª)")
        
        for indicator in self.indicators:
            # åˆå¹¶æ‰€æœ‰æœ‰æ•ˆç»„çš„æ•°æ®è¿›è¡Œç›¸å…³æ€§åˆ†æ
            all_temps = []
            all_values = []
            
            for group in valid_groups:
                group_data = group['data']
                if indicator in group_data.columns:
                    all_temps.extend(group_data['Temperature(K)'].tolist())
                    all_values.extend(group_data[indicator].tolist())
            
            if len(all_temps) < 2:
                continue
            
            # Pearsonç›¸å…³ç³»æ•°ï¼ˆçº¿æ€§ç›¸å…³ï¼‰
            pearson_r, pearson_p = stats.pearsonr(all_temps, all_values)
            # Spearmanç›¸å…³ç³»æ•°ï¼ˆç§©ç›¸å…³ï¼‰
            spearman_r, spearman_p = stats.spearmanr(all_temps, all_values)
            
            significance = 'Yes' if pearson_p < 0.05 else 'No'
            corr_strength = self._get_correlation_strength(abs(pearson_r))
            
            global_temp_results.append({
                'Analysis_Type': 'Controlled_Temperature',
                'Variable': 'Temperature',
                'Indicator': indicator,
                'Sample_Size': len(all_temps),
                'Total_Systems': total_systems,
                'Filtered_Systems': total_filtered_samples,
                'Valid_Groups': len(valid_groups),
                'Filtered_Groups': len(filtered_groups),
                'Temperature_Range': f"{min(unique_temps)}-{max(unique_temps)}K",
                'Pearson_r': pearson_r,
                'Pearson_p': pearson_p,
                'Spearman_r': spearman_r,
                'Spearman_p': spearman_p,
                'Significance': significance
            })
        
        return global_temp_results
    
    def _analyze_global_configuration_effects(self, df: pd.DataFrame) -> List[Dict]:
        """åˆ†ææ„è±¡æ•ˆåº”ï¼ˆæŒ‰å•å˜é‡æ§åˆ¶åŸåˆ™ï¼šå›ºå®šåˆ†å­å’Œæ¸©åº¦ï¼Œåˆ†ææ„è±¡æ•ˆåº”ï¼‰"""
        global_config_results = []
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ„è±¡å˜åŒ–
        unique_configs = df['Configuration'].unique()
        total_systems = len(df)
        
        if not ValidationUtils.validate_sample_size(unique_configs, min_size=2):
            self.logger.warning(f"æ„è±¡ç§ç±»ä¸è¶³({len(unique_configs)}<2)ï¼Œè·³è¿‡æ„è±¡æ•ˆåº”åˆ†æ")
            return global_config_results
        
        # æŒ‰(åˆ†å­,æ¸©åº¦)åˆ†ç»„ï¼Œåªä¿ç•™æ ·æœ¬æ•°>=2çš„ç»„
        valid_groups = []
        filtered_groups = []
        
        for mol_id in df['Molecule_ID'].unique():
            mol_data = df[df['Molecule_ID'] == mol_id]
            for temp in mol_data['Temperature(K)'].unique():
                group_data = mol_data[mol_data['Temperature(K)'] == temp]
                group_data = DataUtils.clean_dataframe(group_data, ['Configuration'] + self.indicators)
                
                if len(group_data) >= 2:
                    valid_groups.append({
                        'molecule': mol_id,
                        'temperature': temp,
                        'data': group_data,
                        'size': len(group_data)
                    })
                else:
                    filtered_groups.append({
                        'molecule': mol_id,
                        'temperature': temp,
                        'size': len(group_data)
                    })
        
        total_valid_samples = sum(group['size'] for group in valid_groups)
        total_filtered_samples = sum(group['size'] for group in filtered_groups)
        
        if not valid_groups:
            self.logger.warning("æ²¡æœ‰æ‰¾åˆ°æ ·æœ¬æ•°>=2çš„(åˆ†å­,æ¸©åº¦)ç»„ï¼Œè·³è¿‡æ„è±¡æ•ˆåº”åˆ†æ")
            return global_config_results
        
        # è®°å½•åˆ†ç»„ä¿¡æ¯
        self.logger.info(f"æ„è±¡åˆ†æåˆ†ç»„æƒ…å†µ:")
        self.logger.info(f"  æœ‰æ•ˆç»„: {len(valid_groups)}ä¸ª")
        if filtered_groups:
            self.logger.info(f"  è¿‡æ»¤ç»„: {len(filtered_groups)}ä¸ª(ç»„å†…æ ·æœ¬ä¸è¶³2ä¸ª)")
        for indicator in self.indicators:
            # å‡†å¤‡å„æ„è±¡ç»„çš„æ•°æ®ï¼ˆæ¥è‡ªæ‰€æœ‰æœ‰æ•ˆç»„ï¼‰
            config_data_dict = {}
            
            for group in valid_groups:
                group_data = group['data']
                if indicator in group_data.columns:
                    for _, row in group_data.iterrows():
                        config = row['Configuration']
                        value = row[indicator]
                        if config not in config_data_dict:
                            config_data_dict[config] = []
                        config_data_dict[config].append(value)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ„è±¡ç»„è¿›è¡Œæ¯”è¾ƒ
            if len(config_data_dict) < 2:
                continue
            
            groups = []
            config_labels = []
            group_sample_counts = []
            
            for config in sorted(config_data_dict.keys()):
                group_data = np.array(config_data_dict[config])
                if len(group_data) > 0:
                    groups.append(group_data)
                    config_labels.append(config)
                    group_sample_counts.append(len(group_data))
            
            # ç¡®ä¿è‡³å°‘æœ‰ä¸¤ä¸ªæœ‰æ•ˆç»„
            if not ValidationUtils.validate_sample_size(groups, min_size=2):
                continue
            
            # æ£€æŸ¥æ¯ç»„çš„æ ·æœ¬é‡ï¼Œæ¯ç»„è‡³å°‘éœ€è¦1ä¸ªæ ·æœ¬æ‰èƒ½è¿›è¡ŒANOVA
            valid_groups_for_anova = [group for group in groups if len(group) >= 1]
            if len(valid_groups_for_anova) < 2:
                continue
            
            # æ‰§è¡Œå•å› ç´ æ–¹å·®åˆ†æ
            f_stat, p_value = stats.f_oneway(*valid_groups_for_anova)
            
            # è®¡ç®—Etaå¹³æ–¹ï¼ˆæ•ˆåº”é‡ï¼‰
            all_values = np.concatenate(valid_groups_for_anova)
            ss_total = np.sum((all_values - all_values.mean()) ** 2)
            ss_between = sum([
                len(group) * (np.mean(group) - all_values.mean()) ** 2 
                for group in valid_groups_for_anova if len(group) > 0
            ])
            eta_squared = DataUtils.safe_divide(ss_between, ss_total, default=0.0)
            
            significance = 'Yes' if p_value < 0.05 else 'No'
            effect_strength = self._get_effect_size_interpretation(eta_squared)
            
            # è®¡ç®—å„ç»„çš„æè¿°æ€§ç»Ÿè®¡
            group_stats = {}
            for i, config in enumerate(config_labels):
                if i < len(valid_groups_for_anova):
                    group_data = valid_groups_for_anova[i]
                    group_stats[f'Config_{config}_mean'] = np.mean(group_data)
                    group_stats[f'Config_{config}_std'] = np.std(group_data)
                    group_stats[f'Config_{config}_n'] = len(group_data)
            
            global_config_results.append({
                'Analysis_Type': 'Controlled_Configuration',
                'Variable': 'Configuration',
                'Indicator': indicator,
                'Sample_Size': len(all_values),
                'Total_Systems': total_systems,
                'Filtered_Systems': total_filtered_samples,
                'Valid_Groups': len(valid_groups),
                'Filtered_Groups': len(filtered_groups),
                'Configurations': sorted(config_labels),
                'F_statistic': f_stat,
                'P_value': p_value,
                'Eta_squared': eta_squared,
                'Significance': significance,
                **group_stats
            })
        
        return global_config_results
    
    def _get_correlation_strength(self, abs_r: float) -> str:
        """è·å–ç›¸å…³æ€§å¼ºåº¦è§£é‡Š"""
        return MathUtils.calculate_correlation_strength(abs_r)
    
    def _get_effect_size_interpretation(self, eta_squared: float) -> str:
        """è·å–æ•ˆåº”é‡è§£é‡Š"""
        return MathUtils.calculate_effect_size_interpretation(eta_squared)
    
    def _get_confidence_level(self, p_value: float) -> str:
        """è·å–ç½®ä¿¡ç¨‹åº¦è¯„ä»·"""
        if p_value < 0.001:
            return "99.9%ç½®ä¿¡"
        elif p_value < 0.01:
            return "99%ç½®ä¿¡"
        elif p_value < 0.05:
            return "95%ç½®ä¿¡"
        elif p_value < 0.1:
            return "90%ç½®ä¿¡"
        else:
            return "ä¸æ˜¾è‘—"
    
    def _save_results(self, temp_results: List[Dict], config_results: List[Dict], global_temp_results: List[Dict], global_config_results: List[Dict], output_dir: str) -> None:
        """ä¿å­˜åˆ†æç»“æœåˆ°CSVæ–‡ä»¶ï¼ˆä»…ä¿å­˜å…¨å±€åˆ†æç»“æœï¼‰"""
        # ä¸»è¦ç»“æœï¼šä»…ä¿å­˜å…¨å±€ç›¸å…³æ€§åˆ†æï¼Œç§»é™¤é‡å¤å­—æ®µç®€åŒ–æ ¼å¼
        main_csv_path = os.path.join(output_dir, "parameter_analysis_results.csv")
        main_data = []
        
        # ä¿å­˜å•å˜é‡æ§åˆ¶æ¸©åº¦ç›¸å…³æ€§ç»“æœ
        for result in global_temp_results:
            main_data.append([
                'Controlled_Temperature_Correlation', result['Indicator'],
                DataUtils.format_number(result['Pearson_r']), 
                DataUtils.format_number(result['Pearson_p']),
                DataUtils.format_number(abs(result['Pearson_r'])),
                result['Significance'],
                self._get_correlation_strength(abs(result['Pearson_r'])),
                f"{result['Sample_Size']}/{result['Total_Systems']}",
                f"Filtered:{result['Filtered_Systems']}; Valid_Groups:{result['Valid_Groups']}; Filtered_Groups:{result['Filtered_Groups']}; Spearman_r={result['Spearman_r']:.3f}; Range={result['Temperature_Range']}"
            ])
        
        # ä¿å­˜å•å˜é‡æ§åˆ¶æ„è±¡æ•ˆåº”ç»“æœ
        for result in global_config_results:
            configs_str = ','.join(map(str, result['Configurations']))
            # æ„å»ºå„ç»„æ ·æœ¬é‡ä¿¡æ¯
            group_sample_info = []
            for config in result['Configurations']:
                if f'Config_{config}_n' in result:
                    group_sample_info.append(f"Config{config}:{result[f'Config_{config}_n']}")
            group_sample_str = ','.join(group_sample_info)
            
            main_data.append([
                'Controlled_Configuration_Effect', result['Indicator'],
                DataUtils.format_number(result['F_statistic']), 
                DataUtils.format_number(result['P_value']),
                DataUtils.format_number(result['Eta_squared']),
                result['Significance'],
                self._get_effect_size_interpretation(result['Eta_squared']),
                f"{result['Sample_Size']}/{result['Total_Systems']}",
                f"Filtered:{result['Filtered_Systems']}; Valid_Groups:{result['Valid_Groups']}; Filtered_Groups:{result['Filtered_Groups']}; Configs=[{configs_str}]; Groups:[{group_sample_str}]"
            ])
        
        # å†™å…¥ä¸»è¦ç»“æœæ–‡ä»¶
        FileUtils.safe_write_csv(
            main_csv_path, main_data,
            headers=[
                'Analysis_Type', 'Indicator', 'Statistic_Value', 'P_value', 
                'Effect_Size', 'Significance', 'Interpretation', 'Valid_Samples', 'Additional_Info'
            ],
            encoding='utf-8'
        )
        
        # æ±‡æ€»è¡¨ï¼šå•å˜é‡æ§åˆ¶åˆ†ææ±‡æ€»
        summary_csv_path = os.path.join(output_dir, "parameter_analysis_summary.csv")
        summary_data = []
        
        # å•å˜é‡æ§åˆ¶æ¸©åº¦ç›¸å…³æ€§æ±‡æ€»
        for result in global_temp_results:
            summary_data.append([
                'Controlled_Temperature_Correlation', result['Indicator'],
                result['Significance'],
                f"{abs(result['Pearson_r']):.3f}",
                f"r={result['Pearson_r']:.3f}, p={result['Pearson_p']:.3f}",
                f"Valid_Groups:{result['Valid_Groups']}, Filtered_Groups:{result['Filtered_Groups']}"
            ])
        
        # å•å˜é‡æ§åˆ¶æ„è±¡æ•ˆåº”æ±‡æ€»
        for result in global_config_results:
            summary_data.append([
                'Controlled_Configuration_Effect', result['Indicator'],
                result['Significance'],
                f"{result['Eta_squared']:.3f}",
                f"F={result['F_statistic']:.3f}, p={result['P_value']:.3f}, Î·Â²={result['Eta_squared']:.3f}",
                f"Valid_Groups:{result['Valid_Groups']}, Filtered_Groups:{result['Filtered_Groups']}"
            ])
        
        # å†™å…¥æ±‡æ€»æ–‡ä»¶
        FileUtils.safe_write_csv(
            summary_csv_path, summary_data,
            headers=[
                'Analysis_Type', 'Indicator', 'Significance', 
                'Effect_Size', 'Statistic_Info', 'Group_Info'
            ],
            encoding='utf-8'
        )
    
    def _log_summary(self, temp_results: List[Dict], config_results: List[Dict], global_temp_results: List[Dict], global_config_results: List[Dict]) -> None:
        """è¾“å‡ºåˆ†ææ€»ç»“"""
        self.logger.info("=" * 50)
        self.logger.info("ç›¸å…³æ€§åˆ†ææ€»ç»“:")
        
        # æ•°æ®æ¦‚è§ˆä¿¡æ¯ï¼ˆä½¿ç”¨æ–°çš„å•å˜é‡æ§åˆ¶æ•°æ®ï¼‰
        if global_temp_results:
            first_temp_result = global_temp_results[0]
            total_samples = first_temp_result['Total_Systems']
            valid_samples_temp = first_temp_result['Sample_Size']
            filtered_samples_temp = first_temp_result['Filtered_Systems']
            valid_groups_temp = first_temp_result['Valid_Groups']
            filtered_groups_temp = first_temp_result['Filtered_Groups']
            temp_range = first_temp_result['Temperature_Range']
            
            self.logger.info(f"æ•°æ®æ¦‚è§ˆ: æ€»è®¡{total_samples}ä¸ªä½“ç³»")
            self.logger.info(f"  æ¸©åº¦åˆ†ææœ‰æ•ˆæ ·æœ¬: {valid_samples_temp}/{total_samples} (è¿‡æ»¤{filtered_samples_temp}ä¸ª)")
            self.logger.info(f"  æ¸©åº¦åˆ†ææœ‰æ•ˆç»„: {valid_groups_temp}ä¸ª(åˆ†å­,æ„è±¡)ç»„ (è¿‡æ»¤{filtered_groups_temp}ä¸ª)")
            self.logger.info(f"  æ¸©åº¦èŒƒå›´: {temp_range}")
        
        if global_config_results:
            first_config_result = global_config_results[0]
            total_samples = first_config_result['Total_Systems']
            valid_samples_config = first_config_result['Sample_Size']
            filtered_samples_config = first_config_result['Filtered_Systems']
            valid_groups_config = first_config_result['Valid_Groups']
            filtered_groups_config = first_config_result['Filtered_Groups']
            configs = first_config_result['Configurations']
            
            self.logger.info(f"  æ„è±¡åˆ†ææœ‰æ•ˆæ ·æœ¬: {valid_samples_config}/{total_samples} (è¿‡æ»¤{filtered_samples_config}ä¸ª)")
            self.logger.info(f"  æ„è±¡åˆ†ææœ‰æ•ˆç»„: {valid_groups_config}ä¸ª(åˆ†å­,æ¸©åº¦)ç»„ (è¿‡æ»¤{filtered_groups_config}ä¸ª)")
            self.logger.info(f"  æ„è±¡ç±»å‹: {self._to_python_list(sorted(configs))}")
        
        # å•å˜é‡æ§åˆ¶æ¸©åº¦ç›¸å…³æ€§åˆ†æ
        if global_temp_results:
            significant_global = [r for r in global_temp_results if r['Significance'] == 'Yes']
            self.logger.info(f"å•å˜é‡æ§åˆ¶æ¸©åº¦ç›¸å…³æ€§: {len(significant_global)}/{len(global_temp_results)}ä¸ªæŒ‡æ ‡æ˜¾è‘—ç›¸å…³")
            
            for result in global_temp_results:
                confidence_level = self._get_confidence_level(result['Pearson_p'])
                corr_strength = self._get_correlation_strength(abs(result['Pearson_r']))
                self.logger.info(f"  {result['Indicator']}: r={result['Pearson_r']:.3f} (p={result['Pearson_p']:.3f}) - {corr_strength}, {confidence_level}")
            
            if significant_global:
                strongest_global = max(significant_global, key=lambda x: abs(x['Pearson_r']))
                self.logger.info(f"  æœ€å¼ºç›¸å…³: {strongest_global['Indicator']} (r={strongest_global['Pearson_r']:.3f})")
        
        # å•å˜é‡æ§åˆ¶æ„è±¡æ•ˆåº”åˆ†æ
        if global_config_results:
            significant_global_config = [r for r in global_config_results if r['Significance'] == 'Yes']
            self.logger.info(f"å•å˜é‡æ§åˆ¶æ„è±¡æ•ˆåº”: {len(significant_global_config)}/{len(global_config_results)}ä¸ªæŒ‡æ ‡æ˜¾è‘—")
            
            for result in global_config_results:
                confidence_level = self._get_confidence_level(result['P_value'])
                effect_strength = self._get_effect_size_interpretation(result['Eta_squared'])
                self.logger.info(f"  {result['Indicator']}: F={result['F_statistic']:.3f} (p={result['P_value']:.3f}), etaÂ²={result['Eta_squared']:.3f} - {effect_strength}, {confidence_level}")
            
            if significant_global_config:
                strongest_config = max(significant_global_config, key=lambda x: x['Eta_squared'])
                self.logger.info(f"  æœ€å¼ºæ•ˆåº”: {strongest_config['Indicator']} (etaÂ²={strongest_config['Eta_squared']:.3f})")
        
        self.logger.info("=" * 50)


def setup_file_logger(output_dir: str) -> logging.Logger:
    """è®¾ç½®æ–‡ä»¶æ—¥å¿—è®°å½•å™¨"""
    # æ—¥å¿—è¾“å‡ºåˆ°analysis_resultsç›®å½•
    analysis_results_dir = os.path.join(os.getcwd(), "analysis_results")
    os.makedirs(analysis_results_dir, exist_ok=True)
    
    logger = logging.getLogger('CorrelationAnalyzer')
    logger.setLevel(logging.INFO)
    
    # æ¸…é™¤ç°æœ‰å¤„ç†å™¨
    logger.handlers.clear()
    
    # æ–‡ä»¶å¤„ç†å™¨ - ä½¿ç”¨æ›´åˆé€‚çš„æ—¥å¿—æ–‡ä»¶å
    log_file = os.path.join(analysis_results_dir, "correlation_analysis.log")
    file_handler = logging.FileHandler(log_file, mode='w', encoding='utf-8')

    # æ§åˆ¶å°å¤„ç†å™¨ï¼Œå¼ºåˆ¶UTF-8ç¼–ç 
    try:
        console_handler = logging.StreamHandler(open(sys.stdout.fileno(), mode='w', encoding='utf-8', buffering=1))
    except Exception:
        # æŸäº›ç¯å¢ƒä¸‹sys.stdout.fileno()ä¸å¯ç”¨ï¼Œé€€å›é»˜è®¤
        console_handler = logging.StreamHandler(sys.stdout)

    # æ ¼å¼è®¾ç½®
    formatter = logging.Formatter(
        '%(asctime)s - %(levelname)s - %(message)s', 
        datefmt='%H:%M:%S'
    )
    file_handler.setFormatter(formatter)
    console_handler.setFormatter(formatter)

    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    logger.propagate = False

    return logger


def find_system_metrics_csv(search_dir: str = ".") -> Optional[str]:
    """
    åœ¨æŒ‡å®šç›®å½•åŠå…¶å­ç›®å½•ä¸­æŸ¥æ‰¾ system_metrics_summary.csv æ–‡ä»¶
    
    Args:
        search_dir: æœç´¢ç›®å½•ï¼Œé»˜è®¤ä¸ºå½“å‰ç›®å½•
        
    Returns:
        str: æ‰¾åˆ°çš„CSVæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å›None
    """
    return FileUtils.find_file_prioritized(
        filename="system_metrics_summary.csv",
        search_dir=search_dir,
        priority_subdirs=["combined_analysis_results"]
    )


def main():
    """ä¸»ç¨‹åºå…¥å£"""
    parser = argparse.ArgumentParser(
        description='ABACUS STRU è½¨è¿¹åˆ†æç›¸å…³æ€§åˆ†æå™¨',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # è‡ªåŠ¨æŸ¥æ‰¾å½“å‰ç›®å½•ä¸‹çš„system_metrics_summary.csvå¹¶åˆ†æï¼ˆé»˜è®¤å¯ç”¨æ—¥å¿—æ–‡ä»¶ï¼‰
  python correlation_analyzer.py
  
  # æŒ‡å®šè¾“å…¥æ–‡ä»¶
  python correlation_analyzer.py -i analysis_results/combined_analysis_results/system_metrics_summary.csv
  
  # æŒ‡å®šè¾“å…¥æ–‡ä»¶å’Œè¾“å‡ºç›®å½•
  python correlation_analyzer.py -i data.csv -o combined_results
  
  # ç¦ç”¨æ—¥å¿—æ–‡ä»¶ï¼Œä»…è¾“å‡ºåˆ°æ§åˆ¶å°
  python correlation_analyzer.py --no-log-file
        """
    )
    
    parser.add_argument(
        '-i', '--input', 
        type=str, 
        help='è¾“å…¥çš„system_metrics_summary.csvæ–‡ä»¶è·¯å¾„ï¼ˆå¦‚ä¸æŒ‡å®šåˆ™è‡ªåŠ¨æŸ¥æ‰¾ï¼‰'
    )
    parser.add_argument(
        '-o', '--output', 
        type=str, 
        default='combined_analysis_results',
        help='è¾“å‡ºç›®å½•è·¯å¾„ï¼ˆé»˜è®¤: combined_analysis_resultsï¼‰'
    )
    parser.add_argument(
        '--no-log-file', 
        action='store_true',
        help='ç¦ç”¨æ—¥å¿—æ–‡ä»¶è¾“å‡ºï¼Œä»…è¾“å‡ºåˆ°æ§åˆ¶å°'
    )
    
    args = parser.parse_args()
    
    # ç¡®å®šè¾“å…¥æ–‡ä»¶è·¯å¾„
    if args.input:
        csv_file_path = args.input
        if not os.path.exists(csv_file_path):
            print(f"é”™è¯¯: æŒ‡å®šçš„è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {csv_file_path}")
            sys.exit(1)
    else:
        # è‡ªåŠ¨æŸ¥æ‰¾
        csv_file_path = find_system_metrics_csv()
        if csv_file_path is None:
            print("é”™è¯¯: æœªæ‰¾åˆ° system_metrics_summary.csv æ–‡ä»¶")
            print("è¯·ä½¿ç”¨ -i å‚æ•°æŒ‡å®šè¾“å…¥æ–‡ä»¶è·¯å¾„")
            sys.exit(1)
        else:
            print(f"è‡ªåŠ¨æ‰¾åˆ°è¾“å…¥æ–‡ä»¶: {csv_file_path}")
    
    # è®¾ç½®æ—¥å¿— - é»˜è®¤å¯ç”¨æ–‡ä»¶æ—¥å¿—
    if args.no_log_file:
        logger = None  # ä»…ä½¿ç”¨é»˜è®¤æ§åˆ¶å°æ—¥å¿—
    else:
        logger = setup_file_logger(args.output)
        # è®°å½•ç‹¬ç«‹è¿è¡Œçš„æ—¥å¿—åˆ°analysis_resultsç›®å½•ï¼Œç¡®ä¿UTF-8ç¼–ç 
        logger.info("å·²å¯ç”¨æ–‡ä»¶æ—¥å¿—è®°å½• (UTF-8 ç¼–ç , è¾“å‡ºåˆ° analysis_results ç›®å½•)")
    
    # åˆ›å»ºåˆ†æå™¨å¹¶æ‰§è¡Œåˆ†æ
    analyzer = CorrelationAnalyzer(logger=logger)
    
    print(f"å¼€å§‹åˆ†ææ–‡ä»¶: {csv_file_path}")
    print(f"è¾“å‡ºç›®å½•: {args.output}")
    if not args.no_log_file:
        print(f"æ—¥å¿—æ–‡ä»¶: analysis_results/correlation_analysis.log")
    
    success = analyzer.analyze_correlations(csv_file_path, args.output)
    
    if success:
        print(f"\nåˆ†æå®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {args.output}")
        print("è¾“å‡ºæ–‡ä»¶:")
        print(f"  - {os.path.join(args.output, 'parameter_analysis_results.csv')}")
        print(f"  - {os.path.join(args.output, 'parameter_analysis_summary.csv')}")
        if not args.no_log_file:
            print(f"  - analysis_results/correlation_analysis.log")
    else:
        print("åˆ†æå¤±è´¥ï¼Œè¯·æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ ¼å¼å’Œå†…å®¹")
        sys.exit(1)


if __name__ == "__main__":
    main()