#!/usr/bin/env python
"""
è„šæœ¬å: correlation_analyser.py
åŠŸèƒ½: ABACUS STRU è½¨è¿¹åˆ†æç›¸å…³æ€§åˆ†æå™¨
==================================================

åŠŸèƒ½ç‰¹æ€§ï¼š
---------
âœ¨ ä¸“ä¸šçš„ç»Ÿ            self.logger.info(f"åˆ†å­: {DataUtils.to_python_types(molecules)}")
            self.logger.info(f"æ„è±¡ç¼–å·: {DataUtils.to_python_types(configs)}")
            self.logger.info(f"æ¸©åº¦: {DataUtils.to_python_types(temperatures)}K")æ¨¡å—ï¼Œç”¨äºåˆ†æ ABACUS STRU è½¨è¿¹æ•°æ®
ğŸ”¬ ç§‘å­¦ä¸¥è°¨çš„ç»Ÿè®¡æ–¹æ³•ï¼Œç¡®ä¿ç»“æœå¯é æ€§
ğŸ“Š æ™ºèƒ½çš„æ ·æœ¬é‡æ£€æŸ¥å’Œè´¨é‡æ§åˆ¶
ğŸ›¡ï¸ ç¨³å¥çš„åˆ†æç­–ç•¥ï¼Œé¿å…å°æ ·æœ¬åè¯¯

ä½¿ç”¨æ–¹å¼ï¼š
---------
1. ç‹¬ç«‹è„šæœ¬è¿è¡Œï¼šåˆ†ææŒ‡å®šçš„ CSV æ–‡ä»¶
2. æ¨¡å—è°ƒç”¨ï¼šé›†æˆåˆ°ä¸»ç¨‹åºåˆ†ææµç¨‹

è¾“å…¥è¦æ±‚ï¼š
---------
- system_metrics_summary.csv æ–‡ä»¶
- åŒ…å«åˆ†å­IDã€æ„è±¡ã€æ¸©åº¦å’Œå„é¡¹æŒ‡æ ‡

è¾“å‡ºç»“æœï¼š
---------
- parameter_analysis_results.csvï¼šå…¨å±€åˆ†æç»“æœï¼ˆæ•´åˆæ•°å€¼å’Œå¯è¯»ä¿¡æ¯ï¼‰
- correlation_analysis.logï¼šåˆ†ææ—¥å¿—

æ ¸å¿ƒç‰¹æ€§ï¼š
---------
- å…¨å±€æ¸©åº¦ç›¸å…³æ€§åˆ†æï¼ˆå¤§æ ·æœ¬ï¼Œé«˜å¯é æ€§ï¼‰
- å…¨å±€æ„è±¡æ•ˆåº”åˆ†æï¼ˆè·¨æ‰€æœ‰åˆ†å­å’Œæ¸©åº¦ï¼‰
- å®Œæ•´çš„ç»Ÿè®¡æ£€éªŒå’Œæ•ˆåº”é‡è¯„ä¼°
- ç®€åŒ–çš„è¾“å‡ºæ ¼å¼ï¼Œèšç„¦å…¨å±€ç»“æœ

ä½œè€…ï¼šLoveElysia1314
ç‰ˆæœ¬ï¼šv3.0
æ—¥æœŸï¼š2025å¹´8æœˆ16æ—¥
æ›´æ–°ï¼šé‡æ„ä¸ºç‹¬ç«‹æ¨¡å—ï¼Œå¢å¼ºç»Ÿè®¡ç¨³å¥æ€§
"""

import argparse
import logging
import os
import sys
from typing import Dict, List, Optional

import numpy as np
import pandas as pd
from scipy import stats

# å¯¼å…¥å·¥å…·æ¨¡å—
from ..utils import (
    Constants,
    DataUtils,
    FileUtils,
    LoggerManager,
    MathUtils,
    ValidationUtils,
)


class CorrelationAnalyser:
    """ç›¸å…³æ€§åˆ†æå™¨ç±»"""

    def __init__(self, logger: Optional[logging.Logger] = None):
        """
        åˆå§‹åŒ–ç›¸å…³æ€§åˆ†æå™¨

        Args:
            logger: æ—¥å¿—è®°å½•å™¨ï¼Œå¦‚æœä¸ºNoneåˆ™åˆ›å»ºé»˜è®¤æ—¥å¿—è®°å½•å™¨
        """
        self.logger = logger if logger is not None else self._create_default_logger()

        # å®šä¹‰åˆ†ææŒ‡æ ‡ï¼ˆåŸå‚æ•°å’Œé‡‡æ ·åå‚æ•°ï¼‰
        self.indicators = [
            "RMSD_Mean",
            "MinD",
            "ANND",
            "MPD",
        ]

    def _create_default_logger(self) -> logging.Logger:
        """åˆ›å»ºé»˜è®¤æ—¥å¿—è®°å½•å™¨"""
        return LoggerManager.create_logger(
            name="CorrelationAnalyser",
            level=logging.INFO,
            add_console=True,
            log_format=Constants.DEFAULT_LOG_FORMAT,
            date_format=Constants.DEFAULT_DATE_FORMAT,
        )

    # è¡¥å……ï¼šç»Ÿä¸€åˆ—è¡¨/æ•°ç»„åˆ°çº¯ Python list çš„å®‰å…¨è½¬æ¢ï¼Œé¿å… AttributeError
    def _to_python_list(self, obj):  # è½»é‡å·¥å…·ï¼Œä¿æŒä¸æ—¥å¿—è°ƒç”¨å…¼å®¹
        try:
            import numpy as np  # å±€éƒ¨å¯¼å…¥ï¼Œé¿å…å…¨å±€ä¾èµ–

            def _convert(x):
                # å°† numpy æ ‡é‡ã€å®‰å…¨ç±»å‹è½¬ä¸ºå†…ç½®ç±»å‹ï¼Œä¿æŒç¨³å®šæ—¥å¿—è¾“å‡º
                if isinstance(x, np.generic):
                    return x.item()
                return x

            if isinstance(obj, (list, tuple, set)):
                return [_convert(x) for x in obj]
            if hasattr(obj, 'tolist'):
                v = obj.tolist()
                if isinstance(v, list):
                    return [_convert(x) for x in v]
                return [_convert(v)]
            return [_convert(obj)]
        except Exception:
            return [str(obj)]



    def analyse_correlations(self, csv_file_path: str, output_dir: str) -> bool:
        """
        åˆ†æåˆå§‹æ„è±¡ã€æ¸©åº¦ä¸å„æŒ‡æ ‡çš„ç›¸å…³æ€§

        Args:
            csv_file_path: system_metrics_summary.csv æ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•è·¯å¾„

        Returns:
            bool: åˆ†ææ˜¯å¦æˆåŠŸå®Œæˆ
        """
        try:
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            FileUtils.ensure_dir(output_dir)

            # å¦‚æœä½¿ç”¨å¤–éƒ¨loggerï¼Œä¸ºç›¸å…³æ€§åˆ†æåˆ›å»ºé¢å¤–çš„æ–‡ä»¶å¤„ç†å™¨
            file_handler = None
            if (
                hasattr(self.logger, "name")
                and self.logger.name != "CorrelationAnalyser"
            ):
                # ä½¿ç”¨å¤–éƒ¨loggeræ—¶ï¼Œæ·»åŠ æ–‡ä»¶è®°å½•åˆ°analysis_resultsç›®å½•
                analysis_results_dir = os.path.join(FileUtils.get_project_root(), "analysis_results")
                os.makedirs(analysis_results_dir, exist_ok=True)
                log_file = os.path.join(
                    analysis_results_dir, "correlation_analysis.log"
                )
                file_handler = LoggerManager.add_file_handler(
                    self.logger,
                    log_file,
                    Constants.DEFAULT_LOG_FORMAT,
                    Constants.DEFAULT_DATE_FORMAT,
                )

            # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not ValidationUtils.validate_file_exists(csv_file_path):
                self.logger.error(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {csv_file_path}")
                return False

            # è¯»å–CSVæ•°æ®
            df = pd.read_csv(csv_file_path)
            self.logger.info(f"æˆåŠŸè¯»å–æ•°æ®æ–‡ä»¶: {csv_file_path}")
            self.logger.info("å¼€å§‹è¿›è¡Œç›¸å…³æ€§åˆ†æï¼ˆéµå¾ªå•ä¸€å˜é‡åŸåˆ™ï¼‰...")

            # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
            required_columns = ["Configuration", "Temperature(K)"] + self.indicators
            missing_columns = DataUtils.check_required_columns(df, required_columns)
            if missing_columns:
                self.logger.error(f"CSVæ–‡ä»¶ç¼ºå°‘å¿…è¦çš„åˆ—: {missing_columns}")
                return False

            # æ£€æŸ¥æ˜¯å¦æœ‰Molecule_IDåˆ—ï¼Œå¦‚æœæ²¡æœ‰åˆ™æ·»åŠ è­¦å‘Š
            if "Molecule_ID" not in df.columns:
                self.logger.warning("æœªæ‰¾åˆ°Molecule_IDåˆ—ï¼Œå°†å‡è®¾æ‰€æœ‰æ•°æ®æ¥è‡ªåŒä¸€åˆ†å­")
                df["Molecule_ID"] = "Unknown"

            # è·å–æ‰€æœ‰åˆ†å­ã€æ„è±¡å’Œæ¸©åº¦ï¼ˆä¿æŒåŸå§‹ç±»å‹ï¼Œé¿å…å¼ºåˆ¶è½¬ä¸ºintå¯¼è‡´é‡å¤æˆ–æ··ä¹±ï¼‰
            molecules = sorted(df["Molecule_ID"].unique())
            configs = sorted(df["Configuration"].unique())
            temperatures = sorted(df["Temperature(K)"].unique())

            self.logger.info(
                f"å‘ç° {len(molecules)} ä¸ªåˆ†å­ã€{len(configs)} ç§æ„è±¡ç¼–å·å’Œ {len(temperatures)} ç§æ¸©åº¦"
            )
            self.logger.info(f"åˆ†å­: {self._to_python_list(molecules)}")
            self.logger.info(f"æ„è±¡ç¼–å·: {self._to_python_list(configs)}")
            self.logger.info(f"æ¸©åº¦: {self._to_python_list(temperatures)}K")

            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            os.makedirs(output_dir, exist_ok=True)

            # æ‰§è¡Œå…¨å±€åˆ†æï¼ˆç§»é™¤åˆ†å­çº§åˆ†æï¼‰
            global_temp_results = self._analyse_global_temperature_correlations(df)
            global_config_results = self._analyse_global_configuration_effects(df)

            # ä¿å­˜ç»“æœï¼ˆä¼ å…¥ç©ºåˆ—è¡¨æ›¿ä»£åˆ†å­çº§ç»“æœï¼‰
            self._save_results(
                [], [], global_temp_results, global_config_results, output_dir
            )
            self.logger.info(f"ç›¸å…³æ€§åˆ†æç»“æœå·²ä¿å­˜åˆ° {output_dir}")

            # è¾“å‡ºæ€»ç»“ï¼ˆä¼ å…¥ç©ºåˆ—è¡¨æ›¿ä»£åˆ†å­çº§ç»“æœï¼‰
            self._log_summary([], [], global_temp_results, global_config_results)

            # æ¸…ç†æ·»åŠ çš„æ–‡ä»¶å¤„ç†å™¨
            if file_handler:
                LoggerManager.remove_handler(self.logger, file_handler)

            return True

        except Exception as e:
            self.logger.error(f"ç›¸å…³æ€§åˆ†æå‡ºé”™: {str(e)}")
            import traceback

            self.logger.error(traceback.format_exc())

            # æ¸…ç†æ·»åŠ çš„æ–‡ä»¶å¤„ç†å™¨
            if "file_handler" in locals() and file_handler:
                LoggerManager.remove_handler(self.logger, file_handler)

            return False

    def _analyse_global_temperature_correlations(self, df: pd.DataFrame) -> List[Dict]:
        """åˆ†ææ¸©åº¦ç›¸å…³æ€§ï¼ˆæŒ‰å•å˜é‡æ§åˆ¶åŸåˆ™ï¼šå›ºå®šåˆ†å­å’Œæ„è±¡ï¼Œåˆ†ææ¸©åº¦æ•ˆåº”ï¼‰"""
        global_temp_results = []

        # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ¸©åº¦å˜åŒ–
        unique_temps = df["Temperature(K)"].unique()
        total_systems = len(df)

        if not ValidationUtils.validate_sample_size(unique_temps, min_size=2):
            self.logger.warning(
                f"æ¸©åº¦ç§ç±»ä¸è¶³({len(unique_temps)}<2)ï¼Œè·³è¿‡æ¸©åº¦ç›¸å…³æ€§åˆ†æ"
            )
            return global_temp_results

        # æŒ‰(åˆ†å­,æ„è±¡)åˆ†ç»„ï¼Œåªä¿ç•™æ ·æœ¬æ•°>=2çš„ç»„
        valid_groups = []
        filtered_groups = []

        for mol_id in df["Molecule_ID"].unique():
            mol_data = df[df["Molecule_ID"] == mol_id]
            for config in mol_data["Configuration"].unique():
                group_data = mol_data[mol_data["Configuration"] == config]
                group_data = DataUtils.clean_dataframe(
                    group_data, ["Temperature(K)"] + self.indicators
                )

                if len(group_data) >= 2:
                    valid_groups.append(
                        {
                            "molecule": mol_id,
                            "config": config,
                            "data": group_data,
                            "size": len(group_data),
                        }
                    )
                else:
                    filtered_groups.append(
                        {"molecule": mol_id, "config": config, "size": len(group_data)}
                    )

        sum(group["size"] for group in valid_groups)
        total_filtered_samples = sum(group["size"] for group in filtered_groups)

        if not valid_groups:
            self.logger.warning("æ²¡æœ‰æ‰¾åˆ°æ ·æœ¬æ•°>=2çš„(åˆ†å­,æ„è±¡)ç»„ï¼Œè·³è¿‡æ¸©åº¦ç›¸å…³æ€§åˆ†æ")
            return global_temp_results

        # è®°å½•åˆ†ç»„ä¿¡æ¯
        self.logger.info("æ¸©åº¦åˆ†æåˆ†ç»„æƒ…å†µ:")
        self.logger.info(f"  æœ‰æ•ˆç»„: {len(valid_groups)}ä¸ª")
        if filtered_groups:
            self.logger.info(f"  è¿‡æ»¤ç»„: {len(filtered_groups)}ä¸ª(ç»„å†…æ ·æœ¬ä¸è¶³2ä¸ª)")

        for indicator in self.indicators:
            # åˆå¹¶æ‰€æœ‰æœ‰æ•ˆç»„çš„æ•°æ®è¿›è¡Œç›¸å…³æ€§åˆ†æ
            all_temps = []
            all_values = []

            for group in valid_groups:
                group_data = group["data"]
                if indicator in group_data.columns:
                    all_temps.extend(group_data["Temperature(K)"].tolist())
                    all_values.extend(group_data[indicator].tolist())

            if len(all_temps) < 2:
                continue

            # Pearsonç›¸å…³ç³»æ•°ï¼ˆçº¿æ€§ç›¸å…³ï¼‰
            try:
                pearson_r, pearson_p = stats.pearsonr(all_temps, all_values)
                if np.isnan(pearson_r) or np.isnan(pearson_p):
                    self.logger.warning(f"æŒ‡æ ‡ '{indicator}' çš„Pearsonç›¸å…³ç³»æ•°è®¡ç®—ç»“æœæ— æ•ˆï¼Œè·³è¿‡")
                    continue
            except Exception as e:
                self.logger.warning(f"æŒ‡æ ‡ '{indicator}' çš„Pearsonç›¸å…³ç³»æ•°è®¡ç®—å¤±è´¥: {str(e)}ï¼Œè·³è¿‡")
                continue

            # Spearmanç›¸å…³ç³»æ•°ï¼ˆç§©ç›¸å…³ï¼‰
            try:
                spearman_r, spearman_p = stats.spearmanr(all_temps, all_values)
                if np.isnan(spearman_r) or np.isnan(spearman_p):
                    self.logger.warning(f"æŒ‡æ ‡ '{indicator}' çš„Spearmanç›¸å…³ç³»æ•°è®¡ç®—ç»“æœæ— æ•ˆï¼Œè·³è¿‡")
                    continue
            except Exception as e:
                self.logger.warning(f"æŒ‡æ ‡ '{indicator}' çš„Spearmanç›¸å…³ç³»æ•°è®¡ç®—å¤±è´¥: {str(e)}ï¼Œè·³è¿‡")
                continue

            significance = "Yes" if pearson_p < 0.05 else "No"

            global_temp_results.append(
                {
                    "Analysis_Type": "Controlled_Temperature",
                    "Variable": "Temperature",
                    "Indicator": indicator,
                    "Sample_Size": len(all_temps),
                    "Total_Systems": total_systems,
                    "Filtered_Systems": total_filtered_samples,
                    "Valid_Groups": len(valid_groups),
                    "Filtered_Groups": len(filtered_groups),
                    "Temperature_Range": f"{min(unique_temps)}-{max(unique_temps)}K",
                    "Pearson_r": pearson_r,
                    "Pearson_p": pearson_p,
                    "Spearman_r": spearman_r,
                    "Spearman_p": spearman_p,
                    "Significance": significance,
                }
            )

        return global_temp_results

    def _analyse_global_configuration_effects(self, df: pd.DataFrame) -> List[Dict]:
        """åˆ†ææ„è±¡æ•ˆåº”ï¼ˆæŒ‰å•å˜é‡æ§åˆ¶åŸåˆ™ï¼šå›ºå®šåˆ†å­å’Œæ¸©åº¦ï¼Œåˆ†ææ„è±¡æ•ˆåº”ï¼‰"""
        global_config_results = []

        # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ„è±¡å˜åŒ–
        unique_configs = df["Configuration"].unique()
        total_systems = len(df)

        if not ValidationUtils.validate_sample_size(unique_configs, min_size=2):
            self.logger.warning(
                f"æ„è±¡ç§ç±»ä¸è¶³({len(unique_configs)}<2)ï¼Œè·³è¿‡æ„è±¡æ•ˆåº”åˆ†æ"
            )
            return global_config_results

        # æŒ‰(åˆ†å­,æ¸©åº¦)åˆ†ç»„ï¼Œåªä¿ç•™æ ·æœ¬æ•°>=2çš„ç»„
        valid_groups = []
        filtered_groups = []

        for mol_id in df["Molecule_ID"].unique():
            mol_data = df[df["Molecule_ID"] == mol_id]
            for temp in mol_data["Temperature(K)"].unique():
                group_data = mol_data[mol_data["Temperature(K)"] == temp]
                group_data = DataUtils.clean_dataframe(
                    group_data, ["Configuration"] + self.indicators
                )

                if len(group_data) >= 2:
                    valid_groups.append(
                        {
                            "molecule": mol_id,
                            "temperature": temp,
                            "data": group_data,
                            "size": len(group_data),
                        }
                    )
                else:
                    filtered_groups.append(
                        {
                            "molecule": mol_id,
                            "temperature": temp,
                            "size": len(group_data),
                        }
                    )

        sum(group["size"] for group in valid_groups)
        total_filtered_samples = sum(group["size"] for group in filtered_groups)

        if not valid_groups:
            self.logger.warning("æ²¡æœ‰æ‰¾åˆ°æ ·æœ¬æ•°>=2çš„(åˆ†å­,æ¸©åº¦)ç»„ï¼Œè·³è¿‡æ„è±¡æ•ˆåº”åˆ†æ")
            return global_config_results

        # è®°å½•åˆ†ç»„ä¿¡æ¯
        self.logger.info("æ„è±¡åˆ†æåˆ†ç»„æƒ…å†µ:")
        self.logger.info(f"  æœ‰æ•ˆç»„: {len(valid_groups)}ä¸ª")
        if filtered_groups:
            self.logger.info(f"  è¿‡æ»¤ç»„: {len(filtered_groups)}ä¸ª(ç»„å†…æ ·æœ¬ä¸è¶³2ä¸ª)")
        for indicator in self.indicators:
            # å‡†å¤‡å„æ„è±¡ç»„çš„æ•°æ®ï¼ˆæ¥è‡ªæ‰€æœ‰æœ‰æ•ˆç»„ï¼‰
            config_data_dict = {}

            for group in valid_groups:
                group_data = group["data"]
                if indicator in group_data.columns:
                    for _, row in group_data.iterrows():
                        config = row["Configuration"]
                        value = row[indicator]
                        if config not in config_data_dict:
                            config_data_dict[config] = []
                        config_data_dict[config].append(value)

            # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ„è±¡ç»„è¿›è¡Œæ¯”è¾ƒ
            if len(config_data_dict) < 2:
                continue

            groups = []
            config_labels = []
            group_sample_counts = []

            for config in sorted(config_data_dict.keys()):
                group_data = np.array(config_data_dict[config])
                if len(group_data) > 0:
                    groups.append(group_data)
                    config_labels.append(config)
                    group_sample_counts.append(len(group_data))

            # ç¡®ä¿è‡³å°‘æœ‰ä¸¤ä¸ªæœ‰æ•ˆç»„
            if not ValidationUtils.validate_sample_size(groups, min_size=2):
                continue

            # æ£€æŸ¥æ¯ç»„çš„æ ·æœ¬é‡ï¼Œæ¯ç»„è‡³å°‘éœ€è¦2ä¸ªæ ·æœ¬æ‰èƒ½è¿›è¡ŒANOVAï¼ˆè®¡ç®—æ–¹å·®ï¼‰
            valid_groups_for_anova = [group for group in groups if len(group) >= 2]
            if len(valid_groups_for_anova) < 2:
                self.logger.warning(f"æŒ‡æ ‡ '{indicator}' çš„æœ‰æ•ˆç»„ä¸è¶³2ä¸ªï¼ˆæ¯ç»„éœ€è¦è‡³å°‘2ä¸ªæ ·æœ¬ï¼‰ï¼Œè·³è¿‡æ„è±¡æ•ˆåº”åˆ†æ")
                continue

            # è®°å½•å“ªäº›æ„è±¡ç»„è¢«è¿‡æ»¤æ‰
            filtered_configs = []
            for i, config in enumerate(config_labels):
                if i >= len(groups) or len(groups[i]) < 2:
                    filtered_configs.append(config)

            if filtered_configs:
                self.logger.info(f"è¿‡æ»¤çš„æ„è±¡ç»„ï¼ˆæ ·æœ¬é‡ä¸è¶³ï¼‰: {filtered_configs}")

            # æ‰§è¡Œå•å› ç´ æ–¹å·®åˆ†æ
            try:
                f_stat, p_value = stats.f_oneway(*valid_groups_for_anova)

                # æ£€æŸ¥ç»“æœæ˜¯å¦æœ‰æ•ˆ
                if np.isnan(f_stat) or np.isnan(p_value):
                    self.logger.warning(f"æŒ‡æ ‡ '{indicator}' çš„æ–¹å·®åˆ†æç»“æœæ— æ•ˆï¼Œè·³è¿‡")
                    continue

            except Exception as e:
                self.logger.warning(f"æŒ‡æ ‡ '{indicator}' çš„æ–¹å·®åˆ†ææ‰§è¡Œå¤±è´¥: {str(e)}ï¼Œè·³è¿‡")
                continue

            # è®¡ç®—Etaå¹³æ–¹ï¼ˆæ•ˆåº”é‡ï¼‰
            all_values = np.concatenate(valid_groups_for_anova)
            ss_total = np.sum((all_values - all_values.mean()) ** 2)
            ss_between = sum(
                [
                    len(group) * (np.mean(group) - all_values.mean()) ** 2
                    for group in valid_groups_for_anova
                    if len(group) > 0
                ]
            )
            eta_squared = DataUtils.safe_divide(ss_between, ss_total, default=0.0)

            significance = "Yes" if p_value < 0.05 else "No"

            # æ›´æ–°æœ‰æ•ˆçš„æ„è±¡æ ‡ç­¾ï¼ˆåªåŒ…å«æœ‰è¶³å¤Ÿæ ·æœ¬çš„ç»„ï¼‰
            valid_config_labels = []
            for i, config in enumerate(config_labels):
                if i < len(groups) and len(groups[i]) >= 2:
                    valid_config_labels.append(config)

            # è®¡ç®—å„ç»„çš„æè¿°æ€§ç»Ÿè®¡
            group_stats = {}
            for i, config in enumerate(valid_config_labels):
                if i < len(valid_groups_for_anova):
                    group_data = valid_groups_for_anova[i]
                    group_stats[f"Config_{config}_mean"] = np.mean(group_data)
                    group_stats[f"Config_{config}_std"] = np.std(group_data, ddof=1) if len(group_data) > 1 else 0.0
                    group_stats[f"Config_{config}_n"] = len(group_data)

            global_config_results.append(
                {
                    "Analysis_Type": "Controlled_Configuration",
                    "Variable": "Configuration",
                    "Indicator": indicator,
                    "Sample_Size": len(all_values),
                    "Total_Systems": total_systems,
                    "Filtered_Systems": total_filtered_samples,
                    "Valid_Groups": len(valid_groups),
                    "Filtered_Groups": len(filtered_groups),
                    "Configurations": valid_config_labels,
                    "F_statistic": f_stat,
                    "P_value": p_value,
                    "Eta_squared": eta_squared,
                    "Significance": significance,
                    **group_stats,
                }
            )

        return global_config_results

    def _get_confidence_level(self, p_value: float) -> str:
        """è·å–ç½®ä¿¡ç¨‹åº¦è¯„ä»·"""
        if p_value < 0.001:
            return "99.9%ç½®ä¿¡"
        elif p_value < 0.01:
            return "99%ç½®ä¿¡"
        elif p_value < 0.05:
            return "95%ç½®ä¿¡"
        elif p_value < 0.1:
            return "90%ç½®ä¿¡"
        else:
            return "ä¸æ˜¾è‘—"

    def _save_results(
        self,
        temp_results: List[Dict],
        config_results: List[Dict],
        global_temp_results: List[Dict],
        global_config_results: List[Dict],
        output_dir: str,
    ) -> None:
        """ä¿å­˜åˆ†æç»“æœåˆ°ç»Ÿä¸€çš„CSVæ–‡ä»¶ï¼ˆç»“æ„åŒ–ã€å»å†—ä½™çš„è¾“å‡ºï¼‰

        è¾“å‡ºåˆ—ï¼ˆè§„èŒƒï¼‰ï¼š
        Analysis_Type, Indicator, Statistic, P_Value, Effect_Size, Significance, Eval,
        Valid_Samples, Total_Systems, Valid_Groups, Filtered_Systems, Spearman_r,
        Temp_Range, Configs, Notes
        """
        main_csv_path = os.path.join(output_dir, "parameter_analysis_results.csv")
        main_data = []

        # æ¸©åº¦ç›¸å…³æ€§ï¼ˆæ¯è¡Œä»£è¡¨ä¸€ä¸ªæŒ‡æ ‡ï¼‰
        for result in global_temp_results:
            r_val = result.get("Pearson_r")
            p_val = result.get("Pearson_p")
            sample_size = result.get("Sample_Size")
            total = result.get("Total_Systems")
            valid_groups = result.get("Valid_Groups")
            filtered_systems = result.get("Filtered_Systems")
            spearman = result.get("Spearman_r")
            temp_range = result.get("Temperature_Range")

            # ç”Ÿæˆå¯è¯»è¯„ä»·ï¼ˆEvalï¼‰å¹¶ç¡®å®šæ˜¾è‘—æ€§ï¼ˆç”±æ•°å€¼å†³å®šï¼‰
            eval_text = self._get_temperature_correlation_evaluation(
                r_val if r_val is not None else float("nan"),
                p_val if p_val is not None else float("nan"),
            )

            notes = []
            # å…¼å®¹å¯èƒ½çš„é€»è¾‘å†²çªï¼ˆCSV ä¸­åŸå…ˆå†™çš„ Interpretationï¼‰
            if "Interpretation" in result and result.get("Interpretation"):
                notes.append(f"orig_interp={result.get('Interpretation')}")

            main_data.append(
                [
                    "Temp_Corr",
                    result.get("Indicator"),
                    DataUtils.format_number(r_val),
                    DataUtils.format_number(p_val),
                    DataUtils.format_number(abs(r_val) if r_val is not None else None),
                    "Yes" if (p_val is not None and not np.isnan(p_val) and p_val < 0.05) else "No",
                    eval_text,
                    int(sample_size) if sample_size is not None else None,
                    int(total) if total is not None else None,
                    int(valid_groups) if valid_groups is not None else None,
                    int(filtered_systems) if filtered_systems is not None else None,
                    DataUtils.format_number(spearman),
                    temp_range,
                    None,
                    "; ".join(notes) if notes else None,
                ]
            )

        # æ„è±¡æ•ˆåº”ï¼ˆANOVAï¼‰
        for result in global_config_results:
            f_stat = result.get("F_statistic")
            p_val = result.get("P_value")
            eta_sq = result.get("Eta_squared")
            sample_size = result.get("Sample_Size")
            total = result.get("Total_Systems")
            valid_groups = result.get("Valid_Groups")
            filtered_systems = result.get("Filtered_Systems")
            configs = result.get("Configurations")

            notes = []
            if p_val is None or (isinstance(p_val, float) and np.isnan(p_val)):
                notes.append("ANOVA not computable: insufficient groups/data")

            eval_text = self._get_configuration_effect_evaluation(
                eta_sq if eta_sq is not None else float("nan"),
                p_val if p_val is not None else float("nan"),
            )

            # å°† configs åºåˆ—åŒ–ä¸ºå­—ç¬¦ä¸²ä»¥ä¾¿ CSV å­˜å‚¨
            configs_str = None
            if configs is not None:
                try:
                    configs_str = ",".join(map(str, configs))
                except Exception:
                    configs_str = str(configs)

            main_data.append(
                [
                    "Config_Effect",
                    result.get("Indicator"),
                    DataUtils.format_number(f_stat),
                    DataUtils.format_number(p_val),
                    DataUtils.format_number(eta_sq),
                    "Yes" if (p_val is not None and not np.isnan(p_val) and p_val < 0.05) else "No",
                    eval_text,
                    int(sample_size) if sample_size is not None else None,
                    int(total) if total is not None else None,
                    int(valid_groups) if valid_groups is not None else None,
                    int(filtered_systems) if filtered_systems is not None else None,
                    None,
                    None,
                    configs_str,
                    "; ".join(notes) if notes else None,
                ]
            )

        # å†™å…¥ç»“æ„åŒ–CSV
        FileUtils.safe_write_csv(
            main_csv_path,
            main_data,
            headers=[
                "Analysis_Type",
                "Indicator",
                "Statistic",
                "P_Value",
                "Effect_Size",
                "Significance",
                "Eval",
                "Valid_Samples",
                "Total_Systems",
                "Valid_Groups",
                "Filtered_Systems",
                "Spearman_r",
                "Temp_Range",
                "Configs",
                "Notes",
            ],
            encoding="utf-8-sig",
        )

    def _log_summary(
        self,
        temp_results: List[Dict],
        config_results: List[Dict],
        global_temp_results: List[Dict],
        global_config_results: List[Dict],
    ) -> None:
        """è¾“å‡ºåˆ†ææ€»ç»“"""
        self.logger.info("=" * 60)
        self.logger.info("ç›¸å…³æ€§åˆ†ææ€»ç»“")
        self.logger.info("=" * 60)

        # æ•°æ®æ¦‚è§ˆä¿¡æ¯ï¼ˆåˆå¹¶é¿å…é‡å¤ï¼‰
        if global_temp_results or global_config_results:
            # ä»ä»»ä¸€ç»“æœä¸­æå–åŸºæœ¬ä¿¡æ¯
            source_result = global_temp_results[0] if global_temp_results else global_config_results[0]
            total_samples = source_result["Total_Systems"]

            self.logger.info(f"æ•°æ®æ¦‚è§ˆ:")
            self.logger.info(f"   æ€»ä½“ç³»æ•°: {total_samples}")

            if global_temp_results:
                first_temp = global_temp_results[0]
                valid_samples_temp = first_temp["Sample_Size"]
                filtered_samples_temp = first_temp["Filtered_Systems"]
                valid_groups_temp = first_temp["Valid_Groups"]
                temp_range = first_temp["Temperature_Range"]
                self.logger.info(f"   æ¸©åº¦åˆ†æ: {valid_samples_temp}/{total_samples}æœ‰æ•ˆæ ·æœ¬ (è¿‡æ»¤{filtered_samples_temp})")
                self.logger.info(f"   æ¸©åº¦èŒƒå›´: {temp_range}")

            if global_config_results:
                first_config = global_config_results[0]
                valid_samples_config = first_config["Sample_Size"]
                filtered_samples_config = first_config["Filtered_Systems"]
                valid_groups_config = first_config["Valid_Groups"]
                configs = first_config["Configurations"]
                self.logger.info(f"   æ„è±¡åˆ†æ: {valid_samples_config}/{total_samples}æœ‰æ•ˆæ ·æœ¬ (è¿‡æ»¤{filtered_samples_config})")
                self.logger.info(f"   æ„è±¡ç±»å‹: {DataUtils.to_python_types(sorted(configs))}")

        # æ¸©åº¦ç›¸å…³æ€§åˆ†æç»“æœ
        if global_temp_results:
            self.logger.info(f"\næ¸©åº¦ç›¸å…³æ€§åˆ†æ:")
            significant_count = sum(1 for r in global_temp_results if r["Significance"] == "Yes")
            self.logger.info(f"   æ˜¾è‘—ç›¸å…³æŒ‡æ ‡: {significant_count}/{len(global_temp_results)}")

            for result in global_temp_results:
                evaluation = self._get_temperature_correlation_evaluation(
                    result["Pearson_r"], result["Pearson_p"]
                )
                self.logger.info(
                    f"   {result['Indicator']:<15} r={result['Pearson_r']:.3f} (p={result['Pearson_p']:.3f}) - {evaluation}"
                )

        # æ„è±¡æ•ˆåº”åˆ†æç»“æœ
        if global_config_results:
            self.logger.info(f"\næ„è±¡æ•ˆåº”åˆ†æ:")
            significant_count = sum(1 for r in global_config_results if r["Significance"] == "Yes")
            self.logger.info(f"   æ˜¾è‘—æ•ˆåº”æŒ‡æ ‡: {significant_count}/{len(global_config_results)}")

            for result in global_config_results:
                # å¤„ç† NaN å€¼
                f_stat = result["F_statistic"]
                p_value = result["P_value"]
                eta_sq = result["Eta_squared"]

                evaluation = self._get_configuration_effect_evaluation(eta_sq, p_value)
                if p_value is not None and not (isinstance(p_value, float) and np.isnan(p_value)) and p_value < 0.05:
                    evaluation += f" (Î·Â²={eta_sq:.3f})"
                
                if np.isnan(f_stat) or np.isnan(p_value):
                    stat_info = "F=nan (p=nan)"
                else:
                    stat_info = f"F={f_stat:.3f} (p={p_value:.3f})"

                self.logger.info(
                    f"   {result['Indicator']:<15} {stat_info}, Î·Â²={eta_sq:.3f} - {evaluation}"
                )

        self.logger.info("=" * 60)

    def _get_temperature_correlation_evaluation(self, r: float, p_value: float) -> str:
        """æ ¹æ®æ–°æ ‡å‡†è‡ªåŠ¨è¯„ä»·æ¸©åº¦ç›¸å…³æ€§"""
        if p_value is None or (isinstance(p_value, float) and np.isnan(p_value)):
            return "æ•°æ®ä¸è¶³ï¼Œæ— æ³•è®¡ç®—"
        
        if r is None or (isinstance(r, float) and np.isnan(r)):
            return "ç›¸å…³ç³»æ•°æ— æ•ˆï¼Œæ— æ³•è®¡ç®—"
        
        if p_value < 0.05:
            abs_r = abs(r)
            if abs_r >= 0.5:
                strength = "å¼ºç›¸å…³"
            elif abs_r >= 0.3:
                strength = "ä¸­ç­‰ç›¸å…³"
            elif abs_r >= 0.1:
                strength = "å¼±ç›¸å…³"
            else:
                strength = "æå¼±ç›¸å…³"
            confidence = self._get_confidence_level(p_value)
            return f"{strength}, {confidence}ç½®ä¿¡"
        else:
            return "æ— ç›¸å…³, ä¸æ˜¾è‘—"

    def _get_configuration_effect_evaluation(self, eta_squared: float, p_value: float) -> str:
        """æ ¹æ®æ–°æ ‡å‡†è‡ªåŠ¨è¯„ä»·æ„è±¡æ•ˆåº”"""
        if p_value is None or (isinstance(p_value, float) and np.isnan(p_value)):
            return "æ•°æ®ä¸è¶³ï¼Œæ— æ³•è®¡ç®—"
        
        if p_value < 0.05:
            if eta_squared >= 0.04:
                effect_size = "ä¸­ç­‰æ•ˆåº”"
            elif eta_squared >= 0.01:
                effect_size = "å°æ•ˆåº”"
            else:
                effect_size = "å¾®å¼±æ•ˆåº”"
            return f"æ˜¾è‘—, {effect_size}"
        else:
            return "æ— æ•ˆåº”, ä¸æ˜¾è‘—"


def setup_file_logger(output_dir: str) -> logging.Logger:
    """è®¾ç½®æ–‡ä»¶æ—¥å¿—è®°å½•å™¨"""
    # ä½¿ç”¨æ–°çš„é›†ä¸­å¼æ—¥å¿—ç®¡ç†å™¨
    return LoggerManager.create_analysis_logger(
        name="CorrelationAnalyser",
        output_dir=output_dir,
        log_filename="correlation_analysis.log"
    )


def find_system_metrics_csv(search_dir: str = ".") -> Optional[str]:
    """
    åœ¨æŒ‡å®šç›®å½•åŠå…¶å­ç›®å½•ä¸­æŸ¥æ‰¾ system_metrics_summary.csv æ–‡ä»¶

    Args:
        search_dir: æœç´¢ç›®å½•ï¼Œé»˜è®¤ä¸ºå½“å‰ç›®å½•

    Returns:
        str: æ‰¾åˆ°çš„CSVæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å›None
    """
    return FileUtils.find_file_prioritized(
        filename="system_metrics_summary.csv",
        search_dir=search_dir,
        priority_subdirs=["combined_analysis_results"],
    )


def main():
    """ä¸»ç¨‹åºå…¥å£"""
    parser = argparse.ArgumentParser(
        description="ABACUS STRU è½¨è¿¹åˆ†æç›¸å…³æ€§åˆ†æå™¨",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # è‡ªåŠ¨æŸ¥æ‰¾å½“å‰ç›®å½•ä¸‹çš„system_metrics_summary.csvå¹¶åˆ†æï¼ˆé»˜è®¤å¯ç”¨æ—¥å¿—æ–‡ä»¶ï¼‰
  python correlation_analyser.py

  # æŒ‡å®šè¾“å…¥æ–‡ä»¶
  python correlation_analyser.py -i analysis_results/combined_analysis_results/system_metrics_summary.csv

  # æŒ‡å®šè¾“å…¥æ–‡ä»¶å’Œè¾“å‡ºç›®å½•
  python correlation_analyser.py -i data.csv -o combined_results

  # ç¦ç”¨æ—¥å¿—æ–‡ä»¶ï¼Œä»…è¾“å‡ºåˆ°æ§åˆ¶å°
  python correlation_analyser.py --no-log-file
        """,
    )

    parser.add_argument(
        "-i",
        "--input",
        type=str,
        help="è¾“å…¥çš„system_metrics_summary.csvæ–‡ä»¶è·¯å¾„ï¼ˆå¦‚ä¸æŒ‡å®šåˆ™è‡ªåŠ¨æŸ¥æ‰¾ï¼‰",
    )
    parser.add_argument(
        "-o",
        "--output",
        type=str,
        default="combined_analysis_results",
        help="è¾“å‡ºç›®å½•è·¯å¾„ï¼ˆé»˜è®¤: combined_analysis_resultsï¼‰",
    )
    parser.add_argument(
        "--no-log-file", action="store_true", help="ç¦ç”¨æ—¥å¿—æ–‡ä»¶è¾“å‡ºï¼Œä»…è¾“å‡ºåˆ°æ§åˆ¶å°"
    )

    args = parser.parse_args()

    # ç¡®å®šè¾“å…¥æ–‡ä»¶è·¯å¾„
    if args.input:
        csv_file_path = args.input
        if not os.path.exists(csv_file_path):
            logger = logging.getLogger(__name__)
            logger.error(f"é”™è¯¯: æŒ‡å®šçš„è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {csv_file_path}")
            sys.exit(1)
    else:
        # è‡ªåŠ¨æŸ¥æ‰¾
        csv_file_path = find_system_metrics_csv()
        if csv_file_path is None:
            logger = logging.getLogger(__name__)
            logger.error("æœªæ‰¾åˆ° system_metrics_summary.csv æ–‡ä»¶")
            logger.error("è¯·ä½¿ç”¨ -i å‚æ•°æŒ‡å®šè¾“å…¥æ–‡ä»¶è·¯å¾„")
            sys.exit(1)
        else:
            logger = logging.getLogger(__name__)
            logger.info(f"è‡ªåŠ¨æ‰¾åˆ°è¾“å…¥æ–‡ä»¶: {csv_file_path}")

    # è®¾ç½®æ—¥å¿— - é»˜è®¤å¯ç”¨æ–‡ä»¶æ—¥å¿—
    if args.no_log_file:
        logger = None  # ä»…ä½¿ç”¨é»˜è®¤æ§åˆ¶å°æ—¥å¿—
    else:
        # ä½¿ç”¨æ–°çš„é›†ä¸­å¼æ—¥å¿—ç®¡ç†å™¨
        analysis_results_dir = os.path.join(FileUtils.get_project_root(), "analysis_results")
        logger = LoggerManager.create_analysis_logger(
            name="CorrelationAnalyser",
            output_dir=analysis_results_dir,
            log_filename="correlation_analysis.log"
        )
        # è®°å½•ç‹¬ç«‹è¿è¡Œçš„æ—¥å¿—åˆ°analysis_resultsç›®å½•ï¼Œç¡®ä¿UTF-8ç¼–ç 
        logger.info("å·²å¯ç”¨æ–‡ä»¶æ—¥å¿—è®°å½• (UTF-8 ç¼–ç , è¾“å‡ºåˆ° analysis_results ç›®å½•)")

    # åˆ›å»ºåˆ†æå™¨å¹¶æ‰§è¡Œåˆ†æ
    analyser = CorrelationAnalyser(logger=logger)

    log_runtime = logging.getLogger("CorrelationAnalyserRuntime")
    log_runtime.info(f"å¼€å§‹åˆ†ææ–‡ä»¶: {csv_file_path}")
    log_runtime.info(f"è¾“å‡ºç›®å½•: {args.output}")
    if not args.no_log_file:
        log_runtime.info("æ—¥å¿—æ–‡ä»¶: analysis_results/correlation_analysis.log")

    success = analyser.analyse_correlations(csv_file_path, args.output)

    if success:
        log_runtime.info(f"åˆ†æå®Œæˆï¼Œç»“æœå·²ä¿å­˜åˆ°: {args.output}")
        log_runtime.info("è¾“å‡ºæ–‡ä»¶:")
        log_runtime.info(f"  - {os.path.join(args.output, 'parameter_analysis_results.csv')} (æ•´åˆæ•°å€¼å’Œå¯è¯»ä¿¡æ¯)")
        if not args.no_log_file:
            log_runtime.info("  - analysis_results/correlation_analysis.log")
    else:
        log_runtime.error("åˆ†æå¤±è´¥ï¼Œè¯·æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ ¼å¼å’Œå†…å®¹")
        sys.exit(1)


if __name__ == "__main__":
    main()
