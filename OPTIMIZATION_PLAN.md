# ä»£ç ä¼˜åŒ–è®¡åˆ’ - æé«˜æ•ˆç‡ï¼Œé™ä½å†—ä½™é€»è¾‘å’Œè¿‡ä¸¥æ ¼éªŒè¯

## æ€§èƒ½ç“¶é¢ˆåˆ†æï¼ˆåŸºäºå½“å‰è¿è¡Œæ—¥å¿—ï¼‰

### æ ¸å¿ƒé—®é¢˜è¯†åˆ«
ä»è¿è¡Œæ—¥å¿—åˆ†æï¼Œ"è¿›ç¨‹æ¨¡å¼ä»»åŠ¡: 3989" ååŠå°æ—¶æ— è¾“å‡ºçš„æ ¹æœ¬åŸå› æ˜¯ï¼š

1. **æµ·é‡æ–‡ä»¶IO**: 3989ä¸ªç³»ç»Ÿ Ã— ~1000å¸§/ç³»ç»Ÿ = 400ä¸‡ä¸ªSTRUæ–‡ä»¶è¯»å–
2. **O(nÂ²)è·ç¦»è®¡ç®—**: æ¯ä¸ªç³»ç»Ÿè®¡ç®—åŸå­é—´è·ç¦»ï¼Œå¤æ‚åº¦æé«˜
3. **ç¼ºä¹è¿›åº¦åé¦ˆ**: ProcessSchedulerä»…æ¯10ç§’è¾“å‡ºä¸€æ¬¡è¿›åº¦
4. **å†…å­˜å‹åŠ›**: åŒæ—¶å¤„ç†å¤šä¸ªå¤§ç³»ç»Ÿçš„æ•°æ®

### ä¼˜åŒ–ä¼˜å…ˆçº§

## ğŸš€ Phase 1: å¿«é€Ÿä¼˜åŒ–ï¼ˆç«‹å³è§æ•ˆï¼‰

### 1. å‡å°‘æ–‡ä»¶éªŒè¯å¼€é”€
**å½“å‰é—®é¢˜**: æ¯ä¸ªSTRUæ–‡ä»¶éƒ½è¿›è¡Œå®Œæ•´æ€§éªŒè¯
```python
# å½“å‰ä»£ç åœ¨stru_parser.py
def parse_file(self, stru_file: str):
    try:
        with open(stru_file, encoding="utf-8") as f:
            lines = f.readlines()  # è¯»å–æ•´ä¸ªæ–‡ä»¶
        # ç„¶åè¿›è¡Œè¯¦ç»†è§£æå’ŒéªŒè¯...
```

**ä¼˜åŒ–æ–¹æ¡ˆ**:
```python
# ä¼˜åŒ–åï¼šè½»é‡çº§é¢„æ£€ + å»¶è¿ŸåŠ è½½
def parse_file_lightweight(self, stru_file: str):
    # åªæ£€æŸ¥æ–‡ä»¶åŸºæœ¬ä¿¡æ¯ï¼Œä¸è¯»å–å†…å®¹
    if not os.path.exists(stru_file):
        return None
    stat = os.stat(stru_file)
    if stat.st_size < 100:  # å¤ªå°çš„æ–‡ä»¶å¯èƒ½æ˜¯æŸåçš„
        return None
    return stru_file  # è¿”å›æ–‡ä»¶è·¯å¾„ï¼Œå»¶è¿Ÿè§£æ
```

### 2. ä¼˜åŒ–è·ç¦»è®¡ç®—ç®—æ³•
**å½“å‰é—®é¢˜**: å®Œæ•´O(nÂ²)è·ç¦»è®¡ç®—
```python
# å½“å‰ä»£ç åœ¨system_analyser.py
for frame in frames:
    dist_vec = MetricCalculator.calculate_distance_vectors(frame.positions)
    # å¯¹æ‰€æœ‰åŸå­å¯¹è®¡ç®—è·ç¦»
```

**ä¼˜åŒ–æ–¹æ¡ˆ**:
```python
# ä¼˜åŒ–1: é‡‡æ ·è®¡ç®—ï¼ˆå‡å°‘è®¡ç®—é‡90%ï¼‰
def calculate_distance_vectors_sampled(positions, sample_ratio=0.1):
    n_atoms = len(positions)
    sample_size = max(100, int(n_atoms * sample_ratio))
    indices = np.random.choice(n_atoms, sample_size, replace=False)
    sampled_positions = positions[indices]
    # åªè®¡ç®—é‡‡æ ·åŸå­çš„è·ç¦»
    return calculate_distance_vectors(sampled_positions)

# ä¼˜åŒ–2: è¿‘ä¼¼è®¡ç®—ï¼ˆä½¿ç”¨kd-treeç­‰ç©ºé—´æ•°æ®ç»“æ„ï¼‰
def calculate_distance_vectors_approximate(positions, max_distance=10.0):
    # åªè®¡ç®—è·ç¦»å°äºé˜ˆå€¼çš„åŸå­å¯¹
    # ä½¿ç”¨ç©ºé—´ç´¢å¼•åŠ é€Ÿ
    pass
```

### 3. å¢åŠ ç»†ç²’åº¦è¿›åº¦åé¦ˆ
**å½“å‰é—®é¢˜**: ä»…æ¯10ç§’è¾“å‡ºä¸€æ¬¡è¿›åº¦
```python
# å½“å‰ProcessScheduler
if now - last_log >= 10 or completed == len(future_map):
    logger.info(f"è¿›åº¦ {completed}/{len(future_map)}")
```

**ä¼˜åŒ–æ–¹æ¡ˆ**:
```python
# ä¼˜åŒ–åï¼šæ¯å¤„ç†å®Œä¸€ä¸ªç³»ç»Ÿå°±è¾“å‡º
def _worker(task, analyser_params):
    start = time.time()
    try:
        # ... å¤„ç†é€»è¾‘ ...
        result = analyser.analyse_system(task.system_path)
        # æ–°å¢ï¼šç«‹å³è¾“å‡ºå®Œæˆä¿¡æ¯
        logger.info(f"âœ“ å®Œæˆç³»ç»Ÿ {task.system_name} ({time.time()-start:.1f}s)")
        return task.system_name, result, time.time() - start
    except Exception as e:
        logger.error(f"âœ— å¤±è´¥ç³»ç»Ÿ {task.system_name}: {e}")
        return task.system_name, (None, str(e)), time.time() - start
```

### 4. å†…å­˜ä¼˜åŒ–
**å½“å‰é—®é¢˜**: åŒæ—¶åŠ è½½æ‰€æœ‰å¸§æ•°æ®
```python
# å½“å‰ä»£ç 
frames = self.parser.parse_trajectory(stru_dir)  # åŠ è½½æ‰€æœ‰å¸§
for frame in frames:  # å¤„ç†æ‰€æœ‰å¸§
    # è®¡ç®—è·ç¦»å‘é‡ç­‰
```

**ä¼˜åŒ–æ–¹æ¡ˆ**:
```python
# ä¼˜åŒ–åï¼šåˆ†æ‰¹å¤„ç†
def parse_trajectory_batched(self, stru_dir, batch_size=100):
    stru_files = glob.glob(os.path.join(stru_dir, "STRU_MD_*"))
    for i in range(0, len(stru_files), batch_size):
        batch_files = stru_files[i:i+batch_size]
        batch_frames = []
        for file in batch_files:
            frame = self.parse_file(file)
            if frame:
                batch_frames.append(frame)
        yield batch_frames  # è¿”å›æ‰¹æ¬¡ï¼Œå¤„ç†å®Œåé‡Šæ”¾å†…å­˜
```

## ğŸ“Š Phase 2: æ ¸å¿ƒç®—æ³•ä¼˜åŒ–

### 1. å®ç°è®¡ç®—ç»“æœç¼“å­˜
```python
class ComputationCache:
    def __init__(self, cache_dir="./cache"):
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)

    def get_cache_key(self, system_path, operation):
        # ç”ŸæˆåŸºäºæ–‡ä»¶ä¿®æ”¹æ—¶é—´çš„ç¼“å­˜é”®
        stat = os.stat(system_path)
        return f"{operation}_{stat.st_mtime}_{hash(system_path)}"

    def load_cached_result(self, key):
        cache_file = os.path.join(self.cache_dir, f"{key}.pkl")
        if os.path.exists(cache_file):
            with open(cache_file, 'rb') as f:
                return pickle.load(f)
        return None

    def save_cached_result(self, key, result):
        cache_file = os.path.join(self.cache_dir, f"{key}.pkl")
        with open(cache_file, 'wb') as f:
            pickle.dump(result, f)
```

### 2. å¹¶è¡Œè®¡ç®—ä¼˜åŒ–
```python
# åœ¨ç³»ç»Ÿå†…éƒ¨ä¹Ÿä½¿ç”¨å¹¶è¡Œ
def analyse_system_parallel(self, system_dir):
    frames = self.parser.parse_trajectory(system_dir)

    # å¹¶è¡Œè®¡ç®—è·ç¦»å‘é‡
    from concurrent.futures import ThreadPoolExecutor
    with ThreadPoolExecutor(max_workers=4) as executor:
        distance_futures = [
            executor.submit(MetricCalculator.calculate_distance_vectors, frame.positions)
            for frame in frames
        ]
        distance_vectors = [f.result() for f in distance_futures]

    # å¹¶è¡Œè®¡ç®—RMSD
    with ThreadPoolExecutor(max_workers=4) as executor:
        rmsd_futures = [
            executor.submit(self._calculate_rmsd_single, frame, mean_structure)
            for frame in frames
        ]
        rmsd_values = [f.result() for f in rmsd_futures]

    return self._build_metrics(distance_vectors, rmsd_values)
```

## ğŸ”§ Phase 3: æ¶æ„çº§ä¼˜åŒ–

### 1. åˆ†å¸ƒå¼å¤„ç†æ¡†æ¶
- ä½¿ç”¨Daskæˆ–Rayå®ç°åˆ†å¸ƒå¼è®¡ç®—
- æ”¯æŒè·¨æœºå™¨æ‰©å±•

### 2. å¢é‡è®¡ç®—æœºåˆ¶
- åªé‡æ–°è®¡ç®—å˜æ›´çš„æ•°æ®
- å®ç°æ™ºèƒ½çš„ä¾èµ–è·Ÿè¸ª

### 3. è‡ªé€‚åº”ç®—æ³•é€‰æ‹©
- æ ¹æ®ç³»ç»Ÿè§„æ¨¡è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜ç®—æ³•
- å®ç°ç®—æ³•A/Bæµ‹è¯•æ¡†æ¶

## ğŸ“ˆ é¢„æœŸæ€§èƒ½æå‡

| ä¼˜åŒ–é˜¶æ®µ | é¢„æœŸæé€Ÿ | ä¸»è¦æ”¹è¿› |
|---------|---------|---------|
| Phase 1 | 2-3å€ | å‡å°‘IOéªŒè¯ï¼Œä¼˜åŒ–ç®—æ³•ï¼Œå¢åŠ è¿›åº¦åé¦ˆ |
| Phase 2 | 5-10å€ | ç¼“å­˜æœºåˆ¶ï¼Œå¹¶è¡Œè®¡ç®—ï¼Œå†…å­˜ä¼˜åŒ– |
| Phase 3 | 10-50å€ | åˆ†å¸ƒå¼å¤„ç†ï¼Œå¢é‡è®¡ç®—ï¼Œæ™ºèƒ½è°ƒåº¦ |

## ğŸ¯ å®æ–½å»ºè®®

### ç«‹å³å¼€å§‹
1. å®ç°è½»é‡çº§æ–‡ä»¶é¢„æ£€
2. å¢åŠ ç»†ç²’åº¦è¿›åº¦æ—¥å¿—
3. æ·»åŠ å†…å­˜ä½¿ç”¨ç›‘æ§

### çŸ­æœŸç›®æ ‡ï¼ˆ1å‘¨å†…ï¼‰
1. å®ç°è®¡ç®—ç»“æœç¼“å­˜
2. ä¼˜åŒ–è·ç¦»è®¡ç®—ç®—æ³•
3. åˆ†æ‰¹å¤„ç†å¤§ç³»ç»Ÿ

### é•¿æœŸè§„åˆ’ï¼ˆ1æœˆå†…ï¼‰
1. åˆ†å¸ƒå¼å¤„ç†æ¡†æ¶
2. å¢é‡è®¡ç®—æœºåˆ¶
3. æ€§èƒ½ç›‘æ§ä»ªè¡¨æ¿

## ğŸ“‹ éªŒè¯æ–¹æ³•

### æ€§èƒ½åŸºå‡†æµ‹è¯•
```python
def benchmark_system(system_path):
    start_time = time.time()

    # æµ‹è¯•ä¸åŒé…ç½®çš„æ€§èƒ½
    configs = [
        {"validation_level": "full", "algorithm": "exact"},
        {"validation_level": "light", "algorithm": "approximate"},
        {"validation_level": "minimal", "algorithm": "sampled"}
    ]

    results = {}
    for config in configs:
        analyser = SystemAnalyser(**config)
        result = analyser.analyse_system(system_path)
        results[str(config)] = time.time() - start_time

    return results
```

### å‡†ç¡®æ€§éªŒè¯
- å¯¹æ¯”ä¼˜åŒ–å‰åç»“æœçš„æ•°å€¼å·®å¼‚
- ç¡®ä¿ä¼˜åŒ–ä¸å½±å“ç§‘å­¦è®¡ç®—çš„å‡†ç¡®æ€§
- å»ºç«‹å›å½’æµ‹è¯•å¥—ä»¶
                if reraise:
                    raise
                return default_return
        return wrapper
    return decorator

# ä½¿ç”¨ç¤ºä¾‹ï¼š
@handle_exceptions(logger=self.logger, default_return=[])
def load_data(self, filepath: str) -> List[dict]:
    return FileUtils.safe_read_json(filepath)
```

## 3. æ•°æ®ç»“æ„ä¼˜åŒ–ï¼ˆæé«˜å†…å­˜å’Œè®¡ç®—æ•ˆç‡ï¼‰

### å½“å‰é—®é¢˜ï¼š
- é¢‘ç¹ä½¿ç”¨ list/dictï¼Œå†…å­˜æ•ˆç‡ä½
- å¤§é‡å¾ªç¯æ“ä½œ
- æ•°æ®è½¬æ¢å¼€é”€å¤§

### ä¼˜åŒ–æ–¹æ¡ˆï¼š
```python
# ä½¿ç”¨æ›´é«˜æ•ˆçš„æ•°æ®ç»“æ„
from collections import defaultdict, deque
from typing import Deque

# ç¤ºä¾‹ï¼šä½¿ç”¨ deque æ›¿ä»£ list è¿›è¡Œé¢‘ç¹çš„ append/popleft æ“ä½œ
class EfficientQueue:
    def __init__(self):
        self._queue: Deque = deque()

    def add_task(self, task):
        self._queue.append(task)

    def get_next_task(self):
        return self._queue.popleft() if self._queue else None

# ä½¿ç”¨ numpy æ•°ç»„æ›¿ä»£ Python list è¿›è¡Œæ•°å€¼è®¡ç®—
import numpy as np

# ç¤ºä¾‹ï¼šæ‰¹é‡å¤„ç†æ•°å€¼æ•°æ®
def process_batch_data(data_list: List[List[float]]) -> np.ndarray:
    """å°†æ•°æ®è½¬æ¢ä¸º numpy æ•°ç»„è¿›è¡Œé«˜æ•ˆå¤„ç†"""
    return np.array(data_list, dtype=np.float32)
```

## 4. ç¼“å­˜æœºåˆ¶ï¼ˆå‡å°‘é‡å¤è®¡ç®—ï¼‰

### å½“å‰é—®é¢˜ï¼š
- é‡å¤è®¡ç®—ç›¸åŒçš„ç»“æœ
- æ–‡ä»¶è¯»å–æ²¡æœ‰ç¼“å­˜
- ç³»ç»Ÿä¿¡æ¯é‡å¤æŸ¥è¯¢

### ä¼˜åŒ–æ–¹æ¡ˆï¼š
```python
# åœ¨ src/utils/common.py ä¸­æ·»åŠ ç¼“å­˜è£…é¥°å™¨
from functools import lru_cache
import time

class CachedFileReader:
    _cache = {}
    _cache_timeout = 300  # 5åˆ†é’Ÿç¼“å­˜

    @classmethod
    @lru_cache(maxsize=128)
    def read_json_cached(cls, filepath: str) -> Optional[dict]:
        """å¸¦ç¼“å­˜çš„ JSON æ–‡ä»¶è¯»å–"""
        cache_key = filepath
        if cache_key in cls._cache:
            cached_time, data = cls._cache[cache_key]
            if time.time() - cached_time < cls._cache_timeout:
                return data

        data = FileUtils.safe_read_json(filepath)
        if data is not None:
            cls._cache[cache_key] = (time.time(), data)
        return data

# ç³»ç»Ÿä¿¡æ¯ç¼“å­˜
@lru_cache(maxsize=100)
def get_system_info_cached(system_path: str) -> dict:
    """ç¼“å­˜ç³»ç»Ÿä¿¡æ¯ï¼Œé¿å…é‡å¤è§£æ"""
    return parse_system_info(system_path)
```

## 5. ç®€åŒ–éªŒè¯é€»è¾‘ï¼ˆå‡å°‘è¿‡ä¸¥æ ¼æ£€æŸ¥ï¼‰

### å½“å‰é—®é¢˜ï¼š
- è¿‡å¤šä¸å¿…è¦çš„æ–‡ä»¶å­˜åœ¨æ€§æ£€æŸ¥
- é‡å¤çš„æ•°æ®éªŒè¯
- æ€§èƒ½å½±å“çš„è°ƒè¯•éªŒè¯

### ä¼˜åŒ–æ–¹æ¡ˆï¼š
```python
# åœ¨ src/utils/common.py ä¸­æ·»åŠ æ¡ä»¶éªŒè¯
class ValidationLevel:
    STRICT = "strict"  # ä¸¥æ ¼éªŒè¯ï¼ˆå¼€å‘/è°ƒè¯•ï¼‰
    NORMAL = "normal"  # æ­£å¸¸éªŒè¯ï¼ˆç”Ÿäº§ï¼‰
    MINIMAL = "minimal"  # æœ€å°éªŒè¯ï¼ˆé«˜æ€§èƒ½ï¼‰

_validation_level = ValidationLevel.NORMAL

def set_validation_level(level: str):
    global _validation_level
    _validation_level = level

def should_validate(level: str) -> bool:
    """æ ¹æ®å½“å‰éªŒè¯çº§åˆ«å†³å®šæ˜¯å¦æ‰§è¡ŒéªŒè¯"""
    levels = {
        ValidationLevel.STRICT: ["strict", "normal", "minimal"],
        ValidationLevel.NORMAL: ["normal", "minimal"],
        ValidationLevel.MINIMAL: ["minimal"]
    }
    return level in levels.get(_validation_level, [])

# ä½¿ç”¨ç¤ºä¾‹ï¼š
def load_system_data(filepath: str) -> dict:
    if should_validate("file_exists"):
        if not os.path.exists(filepath):
            raise FileNotFoundError(f"File not found: {filepath}")

    # æ ¸å¿ƒåŠ è½½é€»è¾‘
    return FileUtils.safe_read_json(filepath)
```

## 6. æ‰¹é‡æ“ä½œä¼˜åŒ–ï¼ˆå‡å°‘IOæ¬¡æ•°ï¼‰

### å½“å‰é—®é¢˜ï¼š
- é¢‘ç¹çš„å°æ–‡ä»¶å†™å…¥
- å•æ¡è®°å½•å¤„ç†
- ç½‘ç»œ/ç£ç›˜IOå¼€é”€å¤§

### ä¼˜åŒ–æ–¹æ¡ˆï¼š
```python
# åœ¨ src/io/result_saver.py ä¸­æ·»åŠ æ‰¹é‡å†™å…¥
class BatchWriter:
    def __init__(self, batch_size: int = 100):
        self.batch_size = batch_size
        self.buffer = []
        self.output_dir = None

    def add_record(self, record: dict):
        self.buffer.append(record)
        if len(self.buffer) >= self.batch_size:
            self.flush()

    def flush(self):
        if not self.buffer:
            return

        # æ‰¹é‡å†™å…¥é€»è¾‘
        try:
            # å°† buffer ä¸­çš„è®°å½•æ‰¹é‡å†™å…¥æ–‡ä»¶
            FileUtils.safe_write_csv(
                os.path.join(self.output_dir, "batch_data.csv"),
                self.buffer,
                headers=["col1", "col2", "col3"]
            )
            self.buffer.clear()
        except Exception as e:
            logger.error(f"æ‰¹é‡å†™å…¥å¤±è´¥: {e}")

# ä½¿ç”¨ç¤ºä¾‹ï¼š
batch_writer = BatchWriter(batch_size=50)
for record in records:
    batch_writer.add_record(record)
batch_writer.flush()  # ç¡®ä¿æœ€åä¸€æ‰¹æ•°æ®è¢«å†™å…¥
```

## 7. å†…å­˜ç®¡ç†ä¼˜åŒ–

### å½“å‰é—®é¢˜ï¼š
- å¤§æ•°æ®é›†å¤„ç†æ—¶å†…å­˜ä½¿ç”¨è¿‡é«˜
- ä¸´æ—¶å¯¹è±¡æ²¡æœ‰åŠæ—¶é‡Šæ”¾
- å†…å­˜æ³„æ¼é£é™©

### ä¼˜åŒ–æ–¹æ¡ˆï¼š
```python
# ä½¿ç”¨ç”Ÿæˆå™¨æ›¿ä»£åˆ—è¡¨
def process_large_dataset(filepaths: List[str]):
    """ä½¿ç”¨ç”Ÿæˆå™¨å¤„ç†å¤§æ–‡ä»¶ï¼Œé¿å…ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰æ•°æ®"""
    for filepath in filepaths:
        data = FileUtils.safe_read_json(filepath)
        if data:
            yield data
        # åŠæ—¶é‡Šæ”¾å†…å­˜
        del data

# å†…å­˜æ± ç®¡ç†
import gc
from contextlib import contextmanager

@contextmanager
def memory_context():
    """å†…å­˜ç®¡ç†ä¸Šä¸‹æ–‡"""
    try:
        yield
    finally:
        gc.collect()  # å¼ºåˆ¶åƒåœ¾å›æ”¶

# ä½¿ç”¨ç¤ºä¾‹ï¼š
with memory_context():
    for data in process_large_dataset(filepaths):
        process_data(data)
```

## 8. æ€§èƒ½ç›‘æ§å’Œåˆ†æ

### å½“å‰é—®é¢˜ï¼š
- ç¼ºä¹æ€§èƒ½ç›‘æ§
- éš¾ä»¥è¯†åˆ«ç“¶é¢ˆ
- ä¼˜åŒ–æ•ˆæœæ— æ³•é‡åŒ–

### ä¼˜åŒ–æ–¹æ¡ˆï¼š
```python
# åœ¨ src/utils/common.py ä¸­æ·»åŠ æ€§èƒ½ç›‘æ§
import time
from functools import wraps

def timed(func):
    @wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()
        duration = end_time - start_time
        logger = logging.getLogger(func.__module__)
        logger.info(f"å‡½æ•° {func.__name__} æ‰§è¡Œæ—¶é—´: {duration:.4f}ç§’")
        return result
    return wrapper

# ä½¿ç”¨ç¤ºä¾‹ï¼š
@timed
def heavy_computation(self):
    # è€—æ—¶æ“ä½œ
    pass

# æ€§èƒ½åˆ†æå·¥å…·
def profile_function(func):
    """ä½¿ç”¨ cProfile åˆ†æå‡½æ•°æ€§èƒ½"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        import cProfile
        import pstats
        from io import StringIO

        pr = cProfile.Profile()
        pr.enable()
        result = func(*args, **kwargs)
        pr.disable()

        s = StringIO()
        sortby = 'cumulative'
        ps = pstats.Stats(pr, stream=s).sort_stats(sortby)
        ps.print_stats()
        logger.info(f"æ€§èƒ½åˆ†æç»“æœ for {func.__name__}:\n{s.getvalue()}")
        return result
    return wrapper
```

## å®æ–½è®¡åˆ’

### Phase 1: åŸºç¡€ä¼˜åŒ–ï¼ˆ1-2å‘¨ï¼‰
1. å®ç°ç»Ÿä¸€æ—¥å¿—è£…é¥°å™¨
2. æ·»åŠ å¼‚å¸¸å¤„ç†è£…é¥°å™¨
3. ä¼˜åŒ–ä¸»è¦æ•°æ®ç»“æ„

### Phase 2: æ€§èƒ½ä¼˜åŒ–ï¼ˆ2-3å‘¨ï¼‰
1. å®ç°ç¼“å­˜æœºåˆ¶
2. æ·»åŠ æ‰¹é‡æ“ä½œ
3. ä¼˜åŒ–å†…å­˜ç®¡ç†

### Phase 3: éªŒè¯ç®€åŒ–ï¼ˆ1å‘¨ï¼‰
1. å®ç°æ¡ä»¶éªŒè¯ç³»ç»Ÿ
2. ç®€åŒ–ä¸å¿…è¦çš„æ£€æŸ¥
3. æ·»åŠ æ€§èƒ½ç›‘æ§

### Phase 4: æµ‹è¯•å’Œè°ƒä¼˜ï¼ˆ1-2å‘¨ï¼‰
1. æ€§èƒ½æµ‹è¯•
2. å†…å­˜åˆ†æ
3. æœ€ç»ˆä¼˜åŒ–

## é¢„æœŸæ•ˆæœ

- **æ•ˆç‡æå‡**: 20-50% çš„æ‰§è¡Œæ—¶é—´å‡å°‘
- **å†…å­˜ä¼˜åŒ–**: 30-60% çš„å†…å­˜ä½¿ç”¨å‡å°‘
- **ä»£ç ç®€åŒ–**: å‡å°‘ 40% çš„é‡å¤ä»£ç 
- **ç»´æŠ¤æ€§**: æé«˜ä»£ç å¯è¯»æ€§å’Œå¯ç»´æŠ¤æ€§
